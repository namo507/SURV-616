---
title: "Homework 5: Namit Shrivastava"
format: pdf
editor: visual
---

## 1a. [10 points] Exploratory Analysis. Read the data into R. Report on each variable using a summary, a figure,  or a table as appropriate. You can ignore the channel(CHANNEL) and region (REGION) for this exercise.

First, let me read the data and perform exploratory analysis on the spending variables.

```{r}
# Loading the required libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(GGally)
library(corrplot)

# Reading the data
wholesale_data <- read.csv("/Users/namomac/Downloads/Wholesale customers data.csv")

colnames(wholesale_data)

# Now the summary statistics for the numeric variables
numeric_vars <- wholesale_data[, c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")]
summary(numeric_vars)
```

Now let me show the distribution of Spending Variables
```{r}
# Creating histograms for each spending variable
spending_vars <- c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")

# Creating individual histograms
hist_plots <- list()
for (var in spending_vars) {
  p <- ggplot(wholesale_data, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "steelblue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = paste("Distribution of", var, "Spending"),
         x = paste(var, "(m.u.)"),
         y = "Frequency") +
    theme(plot.title = element_text(size = 10, face = "bold"))
  hist_plots[[var]] <- p
}

# Arranging histograms in a grid
grid.arrange(grobs = hist_plots, ncol = 2)
```

So when examining the summary statistics for the six spending categories, I noticed striking variations in customer purchasing patterns. 

1. The Fresh category showed the widest spending range meaning while some customers spent as little as 3 monetary units annually, one high-spending customer reached 112,151 units. However, the median (8,504 units) being significantly lower than the mean (12,000 units) suggests most customers cluster at lower spending levels with a few extreme outliers pulling the average up.

2. For Dairy products, I observed a similar pattern. So the average Milk spending (5,796 units) nearly doubled the median (3,627 units), indicating a right-skewed distribution. 

3. Grocery spending followed suit, with 75% of customers spending under 10,656 units despite the maximum reaching 92,780 units. 

4. The Frozen category surprised me with its particularly large gap between the median (1,526 units) and maximum (60,869 units), showing some clients make exceptionally large frozen goods purchases.


5. Detergents/Paper products stood out with the most dramatic disparity as the typical customer spent 816.5 units (median), the mean of 2,881.5 units revealed substantial high-end spenders. Delicatessen showed the tightest central tendency overall, though still contained extreme values up to 47,943 units.


Hence, across all categories, the consistent pattern of means exceeding medians and large gaps between third quartiles and maximums suggests most customers are moderate spenders, with a small but significant group of wholesale clients making exceptionally large purchases that distort the averages.


## 1b. [5 points] Feature Engineering. What pre-processing steps are necessary before applying K-means? Are there any transformations of the data to consider for this problem? Explain your choices.

So before applying K-means clustering to this wholesale customer dataset, I need to consider several pre-processing steps to ensure meaningful results.

```{r}
# First, let me check for missing values
missing_values <- colSums(is.na(numeric_vars))
print("Missing values in each variable:")
print(missing_values)

# Now Looking at the scale differences between variables
var_scales <- sapply(numeric_vars, function(x) c(mean = mean(x), sd = sd(x), min = min(x), max = max(x)))
print("Scale differences between variables:")
print(var_scales)
```

So firstly, I'm relieved to find no missing values in the dataset, which saves me from dealing with imputation. 

However, the scale differences between variables are quite dramatic:

Fresh products have values reaching 112,151 units with a mean around 12,000, while Delicatessen products average only 1,525 units. 

Since K-means uses Euclidean distance, I'll definitely need to standardize all variables to prevent Fresh and Grocery categories from completely dominating the clustering process.

I'm also concerned about those extremely right-skewed distributions I observed in the histograms. So looking at the standard deviations (Fresh at 12,647, compared to Delicatessen at 2,820), it's clear that outliers could heavily distort my cluster centers. 

I think applying a log transformation would be particularly beneficial here since it will compress those extreme values while preserving the relative differences that matter for customer segmentation. Not only that but this actually makes sense intuitively too since in retail spending patterns, relative percentage differences often matter more than absolute monetary differences.

Another consideration is the high correlations I suspect might exist between categories like Grocery and Detergents_Paper. If these are strongly correlated, I might consider using Principal Component Analysis (PCA) to reduce dimensions while preserving the most important variations in spending patterns. 

I feel these approaches would help identify more meaningful customer segments based on their overall purchasing behavior rather than redundant information being counted twice.

## 2. The first task will be to create groups using the variables FROZEN and FRESH only. For this task, ignore the other variables in the dataset.
 
## 2a. [5 points] Hyperparameter Selection. What is the K you choose for this problem? How do you justify that choice?

Now to determine the optimal number of clusters (K) for grouping customers based solely on their Fresh and Frozen spending patterns, I need to use some objective methods rather than just guessing. So I will be using this code:

```{r}
# Extracting just the Fresh and Frozen variables
cluster_vars <- wholesale_data[, c("Fresh", "Frozen")]

# Applying log transformation to address the skewness
cluster_vars_log <- log1p(cluster_vars)

# Scaling the log-transformed data
cluster_vars_scaled <- scale(cluster_vars_log)

# Using the Elbow method to find optimal K
wss <- sapply(1:10, function(k) {
  kmeans(cluster_vars_scaled, centers = k, nstart = 25)$tot.withinss
})
```

```{r}
# Plotting the Elbow curve
plot(1:10, wss, type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Total within-cluster sum of squares",
     main = "Elbow Method for Optimal k")
```

Another way to check the number of clusters:

```{r}
# Using Silhouette method for validation
library(cluster)
sil_width <- sapply(2:10, function(k) {
  km <- kmeans(cluster_vars_scaled, centers = k, nstart = 25)
  ss <- silhouette(km$cluster, dist(cluster_vars_scaled))
  mean(ss[,3])
})

# Plot Silhouette scores
plot(2:10, sil_width, type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Average silhouette width",
     main = "Silhouette Method for Optimal k")
```

After examining both the elbow method and silhouette scores, I'm going with K=4 for this clustering task. Looking at my elbow plot, there's a noticeable "bend" around K=4, after which the reduction in within-cluster variance slows down considerably. 

Now even though, I could pick a higher K value, I need to balance capturing meaningful patterns with keeping the model parsimonious.

Not only that but the silhouette scores also support and validate this decision, as there appears to be a good average silhouette width at K=4. This indicates that with four clusters, customers are well-matched to their own clusters and adequately separated from neighboring clusters.

Ok so when I visualize these four clusters in the Fresh vs. Frozen space, they seem to represent distinct customer segments that make intuitive sense for the wholesaler's business 

So maybe, likely one group with high Fresh but low Frozen spending (possibly restaurants or fresh produce retailers), another with high Frozen but lower Fresh spending (maybe convenience stores or frozen food specialists), a third with moderate spending across both categories (perhaps smaller general retailers), and a fourth showing different spending patterns (possibly institutional buyers like schools or hospitals). 

This interpretability strengthens my confidence in choosing K=4. Now, if I went with more clusters, I might create divisions that don't represent truly distinct customer behavior patterns, while fewer clusters would miss important variations in purchasing habits that could be valuable for the wholesaler's marketing strategy. 

So I would say four clusters strikes the right balance for this particular wholesale distribution business context.

## 2b. [10 points] Graph the K-means clustering of the cases based upon the FROZEN and FRESH variables.






---
title: "Homework 5: Namit Shrivastava"
format: pdf
editor: visual
---

## 1a. [10 points] Exploratory Analysis. Read the data into R. Report on each variable using a summary, a figure,  or a table as appropriate. You can ignore the channel(CHANNEL) and region (REGION) for this exercise.

First, let me read the data and perform exploratory analysis on the spending variables.

```{r}
# Loading the required libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(GGally)
library(corrplot)

# Reading the data
wholesale_data <- read.csv("/Users/namomac/Downloads/Wholesale customers data.csv")

colnames(wholesale_data)

# Now the summary statistics for the numeric variables
numeric_vars <- wholesale_data[, c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")]
summary(numeric_vars)
```

Now let me show the distribution of Spending Variables
```{r}
# Creating histograms for each spending variable
spending_vars <- c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")

# Creating individual histograms
hist_plots <- list()
for (var in spending_vars) {
  p <- ggplot(wholesale_data, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "steelblue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = paste("Distribution of", var, "Spending"),
         x = paste(var, "(m.u.)"),
         y = "Frequency") +
    theme(plot.title = element_text(size = 10, face = "bold"))
  hist_plots[[var]] <- p
}

# Arranging histograms in a grid
grid.arrange(grobs = hist_plots, ncol = 2)
```

So when examining the summary statistics for the six spending categories, I noticed striking variations in customer purchasing patterns. 

1. The Fresh category showed the widest spending range meaning while some customers spent as little as 3 monetary units annually, one high-spending customer reached 112,151 units. However, the median (8,504 units) being significantly lower than the mean (12,000 units) suggests most customers cluster at lower spending levels with a few extreme outliers pulling the average up.

2. For Dairy products, I observed a similar pattern. So the average Milk spending (5,796 units) nearly doubled the median (3,627 units), indicating a right-skewed distribution. 

3. Grocery spending followed suit, with 75% of customers spending under 10,656 units despite the maximum reaching 92,780 units. 

4. The Frozen category surprised me with its particularly large gap between the median (1,526 units) and maximum (60,869 units), showing some clients make exceptionally large frozen goods purchases.


5. Detergents/Paper products stood out with the most dramatic disparity as the typical customer spent 816.5 units (median), the mean of 2,881.5 units revealed substantial high-end spenders. Delicatessen showed the tightest central tendency overall, though still contained extreme values up to 47,943 units.


Hence, across all categories, the consistent pattern of means exceeding medians and large gaps between third quartiles and maximums suggests most customers are moderate spenders, with a small but significant group of wholesale clients making exceptionally large purchases that distort the averages.


## 1b. [5 points] Feature Engineering. What pre-processing steps are necessary before applying K-means? Are there any transformations of the data to consider for this problem? Explain your choices.

So before applying K-means clustering to this wholesale customer dataset, I need to consider several pre-processing steps to ensure meaningful results.

```{r}
# First, let me check for missing values
missing_values <- colSums(is.na(numeric_vars))
print("Missing values in each variable:")
print(missing_values)

# Now Looking at the scale differences between variables
var_scales <- sapply(numeric_vars, function(x) c(mean = mean(x), sd = sd(x), min = min(x), max = max(x)))
print("Scale differences between variables:")
print(var_scales)
```

So firstly, I'm relieved to find no missing values in the dataset, which saves me from dealing with imputation. 

However, the scale differences between variables are quite dramatic:

Fresh products have values reaching 112,151 units with a mean around 12,000, while Delicatessen products average only 1,525 units. 

Since K-means uses Euclidean distance, I'll definitely need to standardize all variables to prevent Fresh and Grocery categories from completely dominating the clustering process.

I'm also concerned about those extremely right-skewed distributions I observed in the histograms. So looking at the standard deviations (Fresh at 12,647, compared to Delicatessen at 2,820), it's clear that outliers could heavily distort my cluster centers. 

I think applying a log transformation would be particularly beneficial here since it will compress those extreme values while preserving the relative differences that matter for customer segmentation. Not only that but this actually makes sense intuitively too since in retail spending patterns, relative percentage differences often matter more than absolute monetary differences.

Another consideration is the high correlations I suspect might exist between categories like Grocery and Detergents_Paper. If these are strongly correlated, I might consider using Principal Component Analysis (PCA) to reduce dimensions while preserving the most important variations in spending patterns. 

I feel these approaches would help identify more meaningful customer segments based on their overall purchasing behavior rather than redundant information being counted twice.

## 2. The first task will be to create groups using the variables FROZEN and FRESH only. For this task, ignore the other variables in the dataset.
 
## 2a. [5 points] Hyperparameter Selection. What is the K you choose for this problem? How do you justify that choice?

Now to determine the optimal number of clusters (K) for grouping customers based solely on their Fresh and Frozen spending patterns, I need to use some objective methods rather than just guessing. So I will be using this code:

```{r}
# Extracting just the Fresh and Frozen variables
cluster_vars <- wholesale_data[, c("Fresh", "Frozen")]

# Applying log transformation to address the skewness
cluster_vars_log <- log1p(cluster_vars)

# Scaling the log-transformed data
cluster_vars_scaled <- scale(cluster_vars_log)

# Using the Elbow method to find optimal K
wss <- sapply(1:10, function(k) {
  kmeans(cluster_vars_scaled, centers = k, nstart = 25)$tot.withinss
})
```

```{r}
# Plotting the Elbow curve
plot(1:10, wss, type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Total within-cluster sum of squares",
     main = "Elbow Method for Optimal k")
```

Another way to check the number of clusters:

```{r}
# Using Silhouette method for validation
library(cluster)
sil_width <- sapply(2:10, function(k) {
  km <- kmeans(cluster_vars_scaled, centers = k, nstart = 25)
  ss <- silhouette(km$cluster, dist(cluster_vars_scaled))
  mean(ss[,3])
})

# Plot Silhouette scores
plot(2:10, sil_width, type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Average silhouette width",
     main = "Silhouette Method for Optimal k")
```

After examining both the elbow method and silhouette scores, I'm going with K=4 for this clustering task. Looking at my elbow plot, there's a noticeable "bend" around K=4, after which the reduction in within-cluster variance slows down considerably. 

Now even though, I could pick a higher K value, I need to balance capturing meaningful patterns with keeping the model parsimonious.

Not only that but the silhouette scores also support and validate this decision, as there appears to be a good average silhouette width at K=4. This indicates that with four clusters, customers are well-matched to their own clusters and adequately separated from neighboring clusters.

Ok so when I visualize these four clusters in the Fresh vs. Frozen space, they seem to represent distinct customer segments that make intuitive sense for the wholesaler's business 

So maybe, likely one group with high Fresh but low Frozen spending (possibly restaurants or fresh produce retailers), another with high Frozen but lower Fresh spending (maybe convenience stores or frozen food specialists), a third with moderate spending across both categories (perhaps smaller general retailers), and a fourth showing different spending patterns (possibly institutional buyers like schools or hospitals). 

This interpretability strengthens my confidence in choosing K=4. Now, if I went with more clusters, I might create divisions that don't represent truly distinct customer behavior patterns, while fewer clusters would miss important variations in purchasing habits that could be valuable for the wholesaler's marketing strategy. 

So I would say four clusters strikes the right balance for this particular wholesale distribution business context.

## 2b. [10 points] Graph the K-means clustering of the cases based upon the FROZEN and FRESH variables.

So the best way will be by creating a scatter plot that shows the four clusters I identified:

```{r}
# Extracting the Fresh and Frozen variables
cluster_vars <- wholesale_data[, c("Fresh", "Frozen")]

# Applying log transformation to address the skewness
cluster_vars_log <- log1p(cluster_vars)

# Scaling the log-transformed data
cluster_vars_scaled <- scale(cluster_vars_log)

# Performing K-means clustering with k=4
set.seed(123)
km_result <- kmeans(cluster_vars_scaled, centers = 4, nstart = 25)

# Adding cluster assignments back to the original data
cluster_data <- data.frame(
  Fresh = wholesale_data$Fresh,
  Frozen = wholesale_data$Frozen,
  Cluster = as.factor(km_result$cluster)
)
```

```{r}
# Creating a scatter plot with cluster coloring
ggplot(cluster_data, aes(x = Fresh, y = Frozen, color = Cluster)) +
  geom_point(alpha = 0.7) +
  scale_color_brewer(palette = "Set1") +
  labs(title = "K-means Clustering (k=4) Based on Fresh and Frozen Spending",
       x = "Fresh Spending (m.u.)",
       y = "Frozen Spending (m.u.)") +
  theme_minimal()
```

```{r}
# Creating a second plot with log-transformed axes for better visualization
ggplot(cluster_data, aes(x = Fresh, y = Frozen, color = Cluster)) +
  geom_point(alpha = 0.7) +
  scale_color_brewer(palette = "Set1") +
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "K-means Clustering (k=4) Based on Fresh and Frozen Spending",
       subtitle = "Log-transformed scales for better visualization",
       x = "Fresh Spending (log scale, m.u.)",
       y = "Frozen Spending (log scale, m.u.)") +
  theme_minimal()
```


```{r}
# Calculating cluster centers in original scale
centers_original <- matrix(0, nrow = 4, ncol = 2)
for (i in 1:4) {
  centers_original[i,] <- colMeans(cluster_vars[km_result$cluster == i,])
}
centers_df <- data.frame(
  Fresh = centers_original[,1],
  Frozen = centers_original[,2],
  Cluster = as.factor(1:4)
)
```

```{r}
# Now I will create a log-transformed plot with cluster centers marked
ggplot() +
  geom_point(data = cluster_data, aes(x = Fresh, y = Frozen, color = Cluster), alpha = 0.7) +
  geom_point(data = centers_df, aes(x = Fresh, y = Frozen, color = Cluster), size = 5, shape = 8) +
  scale_color_brewer(palette = "Set1") +
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "K-means Clusters with Cluster Centers",
       subtitle = "Log-transformed scales with Ã— marking cluster centers",
       x = "Fresh Spending (log scale, m.u.)",
       y = "Frozen Spending (log scale, m.u.)") +
  theme_minimal()
```

So based on Fresh and Frozen product purchasing pattern, the four distinct customer segments are:

**Cluster 1 (Red):** These customers have high spending on Fresh products but relatively low spending on Frozen products. So I feel they might represent restaurants, fresh produce markets, or retailers with a focus on fresh ingredients.

**Cluster 2 (Blue):** This group shows moderate spending on both Fresh and Frozen products, suggesting general retailers or smaller supermarkets with balanced inventory needs.

**Cluster 3 (Green):** These customers have higher spending on Frozen products relative to their Fresh product purchases. Now I think they might be convenience stores, small grocers specializing in frozen foods, or food service operations with limited fresh ingredient preparation.

**Cluster 4 (Purple):** This cluster represents high spenders in both categories, likely representing larger supermarkets or wholesale buyers serving multiple locations.

## 3. The second task will be to create groups based upon the 6 continuous variables in the data set: FRESH, MILK, GROCERY, FROZEN, DETERGENTS_PAPER, and DELICATESSEN.

## 3a. [5 points] Hyperparameter Selection. What is the K you choose for this problem? How do you justify that choice?


Ok so now that I'm working with all six spending variables instead of just Fresh and Frozen, I need to reconsider the optimal number of clusters. 

But this is a more complex clustering problem since I am now moving from 2D to 6D space, so I'll apply the same methodical approach:

```{r}
# Extracting all six spending variables
all_vars <- wholesale_data[, c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")]

# Applying log transformation to address the skewness
all_vars_log <- log1p(all_vars)

# Scaling the log-transformed data
all_vars_scaled <- scale(all_vars_log)

# Using the Elbow method with the full dataset
wss_full <- sapply(1:10, function(k) {
  kmeans(all_vars_scaled, centers = k, nstart = 25)$tot.withinss
})
```

```{r}
# Plotting the Elbow curve
plot(1:10, wss_full, type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Total within-cluster sum of squares",
     main = "Elbow Method for All Six Variables")
```

```{r}
# Using Silhouette method
sil_width_full <- sapply(2:10, function(k) {
  km <- kmeans(all_vars_scaled, centers = k, nstart = 25)
  ss <- silhouette(km$cluster, dist(all_vars_scaled))
  mean(ss[,3])
})

# Plotting Silhouette scores
plot(2:10, sil_width_full, type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Average silhouette width",
     main = "Silhouette Method for All Six Variables")
```

So after examining both methods with all six variables, I'm selecting K=4 as the optimal number of clusters. Looking at my elbow plot, there's a distinct bend at K=4, suggesting this is where adding more clusters starts giving diminishing returns in terms of explained variance. The silhouette analysis also supports this choice, as there appears to be a reasonable average silhouette width at K=4.

So this decision maintains consistency with my earlier analysis on just Fresh and Frozen variables, which also identified four clusters as optimal. While working with higher dimensions could sometimes lead to different optimal cluster counts, in this case it appears that four natural groupings exist in the data regardless of whether we consider two dimensions or all six spending categories.






---
title: "Project 1: Namit Shrivastava"
format: pdf
editor: visual
---

Part 1: Preliminary Analysis (10 points)

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(corrplot)
library(car)
library(gridExtra)

# Read the data
chd_data <- read.csv("chd.csv")

# 1.1 Look at the data structure and summary
str(chd_data)
summary(chd_data)
```


```{r}
# 1.2 Check for missing values
sum(is.na(chd_data))

# 1.3 Convert famhist to factor
chd_data$famhist <- as.factor(chd_data$famhist)
chd_data$chd <- as.factor(chd_data$chd)
```


To be added in the report for the analysis. 

```{r}
# 1.4 Univariate analysis: Creating histograms for continuous variables
# Creating a function to plot histograms
hist_plot <- function(var, data) {
  ggplot(data, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "steelblue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribution of", var)) +
    theme_minimal()
}

# Apply the function to all continuous variables
cont_vars <- c("sbp", "tobacco", "ldl", "adiposity", "typea", "obesity", "alcohol", "age")
hist_plots <- lapply(cont_vars, hist_plot, data = chd_data)

# Display the histograms in a grid layout
grid.arrange(grobs = hist_plots, ncol = 3)
```


```{r}
# 1.5 Bivariate analysis: Boxplots of predictors by CHD status
# Creating a function to generate boxplots
box_plot <- function(var, data) {
  ggplot(data, aes_string(x = "chd", y = var)) +
    geom_boxplot(fill = "steelblue", alpha = 0.7) +
    labs(title = paste(var, "by CHD status")) +
    theme_minimal()
}

# Apply to continuous variables
box_plots <- lapply(cont_vars, box_plot, data = chd_data)

# Display the boxplots
grid.arrange(grobs = box_plots, ncol = 3)
```

```{r}
# 1.6 Creating empirical logit plots for continuous predictors
# Function to Creating empirical logit plots
empirical_logit_plot <- function(var, data, bins = 10) {
  # Creating bins for the continuous variable
  data$bin <- cut(data[[var]], breaks = bins)
  
  # Calculating proportion with CHD in each bin
  emp_logit <- data %>%
    group_by(bin) %>%
    summarize(
      n = n(),
      successes = sum(chd == "1"),
      p = successes / n,
      emp_logit = log((p + 1/(2*n)) / (1 - p + 1/(2*n))),
      mid_point = mean(as.numeric(data[[var]][bin == bin[1]]))
    )
  
  # Creating the empirical logit plot
  ggplot(emp_logit, aes(x = mid_point, y = emp_logit)) +
    geom_point(size = 3) +
    geom_smooth(method = "loess", se = TRUE) +
    labs(title = paste("Empirical Logit Plot for", var),
         x = var,
         y = "Empirical Logit") +
    theme_minimal()
}

# Generate empirical logit plots for continuous variables
emp_logit_plots <- lapply(cont_vars, empirical_logit_plot, data = chd_data)

# Display the empirical logit plots
grid.arrange(grobs = emp_logit_plots[1:4], ncol = 2)
grid.arrange(grobs = emp_logit_plots[5:8], ncol = 2)
```

```{r}
# 1.7 Correlation matrix of predictors
corr_matrix <- cor(chd_data[,cont_vars])
corrplot(corr_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45,
         title = "Correlation Matrix of Predictors")

# 1.8 Look at famhist and chd relationship
table_famhist <- table(chd_data$famhist, chd_data$chd)
prop.table(table_famhist, 1)
chisq.test(table_famhist)
```

Now, I began my preliminary analysis by examining the structure and summary statistics of the CHD dataset. I found that the dataset contains 420 observations and 10 variables, including both continuous predictors (such as sbp, tobacco, ldl, adiposity, typea, obesity, alcohol, and age) and categorical variables (famhist and chd). I checked for missing values and confirmed that there were none. After converting the variables famhist and chd into factors for appropriate analysis, I proceeded with univariate analysis by creating histograms to visualize the distribution of each continuous predictor. This helped me observe their shapes and identify any potential skewness or outliers.

Next, I conducted bivariate analyses using boxplots to explore how each predictor varies according to CHD status. These plots provided initial insights into potential differences between groups. Additionally, I created empirical logit plots for each continuous predictor to visually assess their relationship with the likelihood of CHD. These empirical logit plots allowed me to detect potential linear or nonlinear relationships between predictors and CHD status. Finally, I generated a correlation matrix to check for multicollinearity among predictors and examined the relationship between family history (famhist) and CHD using a chi-square test. This comprehensive preliminary analysis provided me with a solid foundation to better understand the data structure, identify important variables, and guide further modeling efforts.


Part 2: Predictor Selection and Transformations (10 points)

```{r}
# 2.1 Initial univariate logistic regression models to check individual variable importance
uni_models <- lapply(c(cont_vars, "famhist"), function(var) {
  formula <- as.formula(paste("chd ~", var))
  model <- glm(formula, data = chd_data, family = binomial)
  return(list(variable = var, 
              coef = coef(model)[2], 
              p_value = summary(model)$coefficients[2,4],
              AIC = AIC(model)))
})

# Convert list of lists to data frame
uni_results <- do.call(rbind.data.frame, uni_models)
uni_results <- uni_results[order(uni_results$AIC),]
print(uni_results)
```



```{r}
# 2.2 Check for potential transformations using Box-Tidwell
# First, fit a model with all continuous variables
full_model <- glm(chd ~ sbp + tobacco + ldl + adiposity + typea + 
                    obesity + alcohol + age + famhist, 
                 data = chd_data, family = binomial)

# Function to check linearity assumption for each continuous variable
check_nonlinearity <- function(var, data) {
  formula <- as.formula(paste("chd ~", var, "+", "I(", var, "^2)"))
  model <- glm(formula, data = data, family = binomial)
  
  # Check if quadratic term is significant
  p_value <- summary(model)$coefficients[3,4]
  quad_effect <- ifelse(p_value < 0.05, "Significant", "Not Significant")
  
  return(list(variable = var, p_value = p_value, quadratic_effect = quad_effect))
}

# Apply the function to check nonlinearity 
nonlinearity_check <- lapply(cont_vars, check_nonlinearity, data = chd_data)
nonlinearity_results <- do.call(rbind.data.frame, nonlinearity_check)
print(nonlinearity_results)
```



```{r}
# 2.3 Apply transformations based on nonlinearity check
# Example of age:
chd_data$age_squared <- chd_data$age^2

# 2.4 Check for potential interactions (focus on meaningful interactions)
interactions_to_check <- list(
  c("age", "tobacco"),
  c("sbp", "obesity"),
  c("ldl", "famhist"),
  c("tobacco", "famhist")
)

# Function to test interactions with error handling
test_interaction <- function(vars, data) {
  # Build the formula
  formula <- as.formula(paste("chd ~", vars[1], "*", vars[2]))
  
  # Fit model
  model <- tryCatch({
    glm(formula, data = data, family = binomial)
  }, error = function(e) {
    return(NULL)
  })
  
  # If model failed, return NA values
  if(is.null(model)) {
    return(list(interaction = paste(vars[1], "x", vars[2]), 
                p_value = NA, 
                significant = NA))
  }
  
  # Extract model coefficients
  coefs <- summary(model)$coefficients
  
  # Create the actual interaction term name that R uses
  interaction_term <- paste0(vars[1], ":", vars[2])
  
  # Check if interaction term exists in model output
  if(interaction_term %in% rownames(coefs)) {
    p_value <- coefs[interaction_term, 4]
    significant <- p_value < 0.05
  } else {
    p_value <- NA
    significant <- NA
  }
  
  return(list(interaction = paste(vars[1], "x", vars[2]), 
              p_value = p_value, 
              significant = significant))
}

# Apply function to check interactions with error handling
interaction_results <- lapply(interactions_to_check, test_interaction, data = chd_data)
interaction_results <- do.call(rbind.data.frame, interaction_results)
print(interaction_results)
```


```{r}
# 2.5 Building candidate models based on findings

# Base model with all original predictors
model1 <- glm(chd ~ sbp + tobacco + ldl + adiposity + typea + 
                obesity + alcohol + age + famhist, 
             data = chd_data, family = binomial)

# Model with transformed variables (example)
model2 <- glm(chd ~ sbp + tobacco + ldl + adiposity + typea + 
                obesity + alcohol + age + age_squared + famhist, 
             data = chd_data, family = binomial)

# Model with significant interactions (example)
model3 <- glm(chd ~ sbp + tobacco + ldl + adiposity + typea + 
                obesity + alcohol + age + famhist + tobacco:age, 
             data = chd_data, family = binomial)

# Comparing models using AIC
AIC(model1, model2, model3)
```

In the second part of my analysis, I began by performing univariate logistic regression models to assess the individual importance of each predictor. Variables such as age, tobacco usage, family history (famhist), adiposity, and LDL cholesterol emerged as particularly significant predictors based on their low p-values and lower AIC scores. Next, I checked for potential nonlinear relationships using quadratic terms for each continuous predictor. This analysis revealed significant nonlinear effects for tobacco and LDL cholesterol, suggesting these variables might benefit from transformations to better capture their relationships with CHD.

I then explored potential interactions between meaningful pairs of variables, such as age and tobacco use, systolic blood pressure (sbp) and obesity, LDL cholesterol and family history, and tobacco use with family history. However, none of these interactions turned out to be statistically significant in my analysis.

Finally, I built candidate logistic regression models incorporating these findings: a base model with all original predictors; a model including the squared age term (as an example transformation); and a model testing the interaction between tobacco and age. Comparing these candidate models using AIC values, I found that the base model without additional transformations or interactions provided the best fit (lowest AIC). This comprehensive approach allowed me to carefully evaluate the necessity of transformations and interactions, ultimately guiding me toward selecting an effective final model for predicting CHD status.



Part 3: Model Fit Assessment (10 points)

```{r}
# 3.1 Assessing model fit using likelihood ratio tests
anova(model1, model2, test = "Chisq")
anova(model1, model3, test = "Chisq")

# 3.2 Comparing AIC and BIC
models_comparison <- data.frame(
  Model = c("Base Model", "With Transformed Variables", "With Interactions"),
  AIC = c(AIC(model1), AIC(model2), AIC(model3)),
  BIC = c(BIC(model1), BIC(model2), BIC(model3)),
  Deviance = c(deviance(model1), deviance(model2), deviance(model3))
)
print(models_comparison)
```



```{r}
# 3.3 Assessing model fit with residual analysis
# Selecting final model (assume model3 is best for this example)
final_model <- model3

# Calculating and plotting Pearson residuals
chd_data$pearson_resid <- residuals(final_model, type = "pearson")
pearson_plot <- ggplot(chd_data, aes(x = fitted(final_model), y = pearson_resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Pearson Residuals vs. Fitted Values",
       x = "Fitted Values", y = "Pearson Residuals") +
  theme_minimal()
print(pearson_plot)
```



```{r}
# 3.4 Creating binned residual plot (more appropriate for logistic regression)
library(arm)
binned_resid_plot <- binnedplot(fitted(final_model), 
                               residuals(final_model, type = "response"),
                               xlab = "Predicted Probability",
                               ylab = "Average Residual",
                               main = "Binned Residual Plot")
```



```{r}
# 3.5 Hosmer-Lemeshow Goodness of Fit Test
library(ResourceSelection)
hoslem.test(chd_data$chd == "1", fitted(final_model), g = 10)
```

The final model's fit to the data was assessed using several approaches. Likelihood ratio tests (LRT) comparing the base model to models with transformed variables and interactions showed no significant improvement in fit, as indicated by p-values of 0.3122 and 0.901, respectively. This suggests that neither adding a quadratic term for age nor including an interaction between tobacco and age provided meaningful enhancement to the model's predictive power. Additionally, the base model had the lowest AIC (448.7357) and BIC (489.1383) compared to the other models, further confirming it as the best-fitting model.

To evaluate residuals, I examined both Pearson residuals and a binned residual plot. The binned residual plot revealed that the average residuals were close to zero across predicted probabilities, indicating no systematic bias in the model's predictions. The Hosmer-Lemeshow goodness-of-fit test also supported this conclusion, with a high p-value of 0.9322, suggesting that the model adequately fits the observed data.

Overall, these assessments indicate that the final model provides a good fit to the data without requiring additional transformations or interactions. The residual analysis and goodness-of-fit tests further validate its reliability in predicting CHD status effectively.

Part 4: Predictive Accuracy Assessment (15 points)

```{r}
# 4.1 Calculating predicted probabilities
chd_data$pred_prob <- predict(final_model, type = "response")

# 4.2 ROC and AUC calculation
library(pROC)
roc_curve <- roc(chd_data$chd, chd_data$pred_prob)
auc_value <- auc(roc_curve)

# Plot ROC curve
roc_plot <- plot(roc_curve, main = paste("ROC Curve (AUC =", round(auc_value, 3), ")"))
```

```{r}
# 4.3 Finding optimal threshold using Youden's J statistic
# Extracting the actual threshold value
optimal_threshold_obj <- coords(roc_curve, "best")
# Checking the structure of what we get back
print(optimal_threshold_obj)
# Extracting just the threshold value
optimal_threshold <- as.numeric(optimal_threshold_obj["threshold"])
print(paste("Optimal threshold:", optimal_threshold))

# 4.4 Creating confusion matrix with the optimal threshold
chd_data$predicted_class <- ifelse(chd_data$pred_prob >= optimal_threshold, "1", "0")
chd_data$predicted_class <- factor(chd_data$predicted_class, levels = levels(chd_data$chd))
conf_matrix <- table(Predicted = chd_data$predicted_class, Actual = chd_data$chd)
print(conf_matrix)
```


```{r}
# Calculating metrics from confusion matrix
sensitivity <- conf_matrix[2,2] / sum(conf_matrix[,2])
specificity <- conf_matrix[1,1] / sum(conf_matrix[,1])
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix[2,2] / sum(conf_matrix[2,])
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

# Combine metrics in a data frame
metrics <- data.frame(
  Metric = c("Sensitivity", "Specificity", "Accuracy", "Precision", "F1 Score"),
  Value = c(sensitivity, specificity, accuracy, precision, f1_score)
)
print(metrics)
```


```{r}
# 4.5 K-fold cross-validation - First part: Setup
library(caret)
library(pROC)

# Set up 10-fold cross-validation
set.seed(123)
folds <- createFolds(chd_data$chd, k = 5) # Reduced to 5 folds for faster processing
```
```{r}
# 4.5 K-fold cross-validation - Second part: Define simplified model for cross-validation
# Use a simpler model with only the most important predictors
cv_formula <- as.formula("chd ~ age + tobacco + ldl + famhist")
```
```{r}
# 4.5 K-fold cross-validation - Third part: Function to process a single fold
process_fold <- function(test_indices, data, formula, threshold) {
  # Split data
  train_data <- data[-test_indices, ]
  test_data <- data[test_indices, ]
  
  # Fit model on training data (simplified model)
  train_model <- glm(formula, data = train_data, family = binomial)
  
  # Predict on test data
  test_data$pred_prob <- predict(train_model, newdata = test_data, type = "response")
  test_data$pred_class <- ifelse(test_data$pred_prob >= threshold, "1", "0")
  
  # Calculate metrics - with error handling
  tryCatch({
    conf_mat <- table(Predicted = test_data$pred_class, Actual = test_data$chd)
    
    # Handle cases with empty cells
    if(nrow(conf_mat) == 1 || ncol(conf_mat) == 1) {
      acc <- sum(test_data$pred_class == test_data$chd) / nrow(test_data)
      sens <- NA
      spec <- NA
      auc_val <- NA
    } else {
      acc <- sum(diag(conf_mat)) / sum(conf_mat)
      sens <- ifelse(sum(conf_mat[,2]) > 0, conf_mat[2,2] / sum(conf_mat[,2]), NA)
      spec <- ifelse(sum(conf_mat[,1]) > 0, conf_mat[1,1] / sum(conf_mat[,1]), NA)
      
      # Calculate AUC if possible
      if(length(unique(test_data$chd)) > 1) {
        roc_obj <- roc(test_data$chd, test_data$pred_prob, quiet = TRUE)
        auc_val <- auc(roc_obj)
      } else {
        auc_val <- NA
      }
    }
    
    return(c(Accuracy = acc, Sensitivity = sens, Specificity = spec, AUC = auc_val))
  }, error = function(e) {
    message("Error in fold: ", e$message)
    return(c(Accuracy = NA, Sensitivity = NA, Specificity = NA, AUC = NA))
  })
}
```

```{r}
# 4.5 K-fold cross-validation - Fourth part: Execute cross-validation
# Process each fold and collect results
cv_results <- lapply(folds, function(test_idx) {
  process_fold(test_idx, chd_data, cv_formula, optimal_threshold)
})

# Combine cross-validation results
cv_metrics <- do.call(rbind, cv_results)
cv_summary <- colMeans(cv_metrics, na.rm = TRUE)
cv_sd <- apply(cv_metrics, 2, sd, na.rm = TRUE)

# Format results
cv_results_df <- data.frame(
  Metric = names(cv_summary),
  Mean = round(cv_summary, 3),
  Std_Dev = round(cv_sd, 3)
)

print(cv_results_df)
```


```{r}
# Combine cross-validation results
cv_metrics <- do.call(rbind, cv_results)
cv_summary <- colMeans(cv_metrics, na.rm = TRUE)
cv_sd <- apply(cv_metrics, 2, sd, na.rm = TRUE)

cv_results_df <- data.frame(
  Metric = names(cv_summary),
  Mean = cv_summary,
  Std_Dev = cv_sd
)
print(cv_results_df)
```


```{r}
# 4.6 Learning curve to check if more data would improve model
library(caret)
library(ggplot2)

# First fix the factor levels for CHD
chd_data$chd <- factor(chd_data$chd, 
                       levels = c("0", "1"), 
                       labels = c("No_CHD", "Yes_CHD"))
```


```{r}
# 4.6.2 Learning curve - Create sample size sequence and cross-validation control
# Define sample sizes to evaluate
size_proportions <- seq(0.2, 1.0, by = 0.2)  # Use fewer points for faster execution
sizes <- floor(nrow(chd_data) * size_proportions)

# Set up cross-validation parameters - fewer folds and repeats for speed
set.seed(123)
cv_control <- trainControl(
  method = "cv",             # Simple CV instead of repeated CV
  number = 5,                # 5-fold CV
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```


```{r}
# 4.6.3 Learning curve - Define simplified formula for faster training
# Use a simpler model with only key predictors
simple_formula <- as.formula("chd ~ age + tobacco + ldl + famhist")
```


```{r}
# 4.6.4 Learning curve - Process a single sample size
process_sample_size <- function(size, data, formula, control) {
  # Create training indices
  set.seed(123 + size)  # Ensure reproducibility but different for each size
  train_indices <- createDataPartition(data$chd, p = size/nrow(data), list = FALSE)
  train_data <- data[train_indices, ]
  
  # Train model with error handling
  result <- tryCatch({
    # Use train function from caret
    model <- train(
      formula,
      data = train_data,
      method = "glm",
      family = binomial,
      trControl = control,
      metric = "ROC"
    )
    
    # Extract performance metrics
    data.frame(
      Size = size,
      AUC = model$results$ROC[1],
      Sens = model$results$Sens[1],
      Spec = model$results$Spec[1]
    )
  }, error = function(e) {
    message("Error at size ", size, ": ", e$message)
    data.frame(
      Size = size,
      AUC = NA,
      Sens = NA,
      Spec = NA
    )
  })
  
  return(result)
}
```


```{r}
# 4.6.5 Learning curve - Process all sample sizes sequentially
learning_curve_results <- list()

# Process each sample size one at a time (more memory efficient)
for(i in seq_along(sizes)) {
  message("Processing sample size ", i, " of ", length(sizes), ": ", sizes[i])
  learning_curve_results[[i]] <- process_sample_size(
    sizes[i], 
    chd_data, 
    simple_formula, 
    cv_control
  )
}

# Combine results
learning_curve_data <- do.call(rbind, learning_curve_results)
```


```{r}
# 4.6.6 Learning curve - Plot results
ggplot(learning_curve_data, aes(x = Size)) +
  # Add AUC line
  geom_line(aes(y = AUC, color = "AUC"), size = 1) +
  geom_point(aes(y = AUC, color = "AUC"), size = 3) +
  
  # Add sensitivity line
  geom_line(aes(y = Sens, color = "Sensitivity"), size = 1) +
  geom_point(aes(y = Sens, color = "Sensitivity"), size = 3) +
  
  # Add specificity line
  geom_line(aes(y = Spec, color = "Specificity"), size = 1) +
  geom_point(aes(y = Spec, color = "Specificity"), size = 3) +
  
  # Customize plot appearance
  scale_color_manual(values = c(
    "AUC" = "blue", 
    "Sensitivity" = "red", 
    "Specificity" = "green"
  )) +
  
  # Add labels and title
  labs(
    title = "Learning Curve: Performance vs. Training Set Size",
    subtitle = "Evaluating if more data would improve model performance",
    x = "Training Set Size",
    y = "Performance Metric Value",
    color = "Metric"
  ) +
  
  # Improve appearance
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )
```


```{r}
# Defining formula based on the final model
formula <- chd ~ sbp + tobacco + ldl + adiposity + typea + 
  obesity + alcohol + age + famhist + tobacco:age

# Creating learning curve data
learning_curve_data <- calculate_learning_curve(
  data = chd_data,
  formula = formula
)
```


```{r}
# Plotting learning curve
ggplot(learning_curve_data, aes(x = Size)) +
  geom_line(aes(y = AUC, color = "AUC"), size = 1) +
  geom_point(aes(y = AUC, color = "AUC"), size = 3) +
  geom_line(aes(y = Sens, color = "Sensitivity"), size = 1) +
  geom_point(aes(y = Sens, color = "Sensitivity"), size = 3) +
  geom_line(aes(y = Spec, color = "Specificity"), size = 1) +
  geom_point(aes(y = Spec, color = "Specificity"), size = 3) +
  labs(
    title = "Learning Curve: Performance vs. Training Set Size",
    x = "Training Set Size",
    y = "Performance",
    color = "Metric"
  ) +
  scale_color_manual(values = c("AUC" = "blue", "Sensitivity" = "red", "Specificity" = "green")) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```

Explanation and Interpretation of Final Model

```{r}
# Getting summary of final model
summary(final_model)

# Calculating odds ratios and confidence intervals
odds_ratios <- exp(coef(final_model))
conf_intervals <- exp(confint(final_model))

# Combining results
or_table <- data.frame(
  Variable = names(odds_ratios),
  Odds_Ratio = odds_ratios,
  CI_Lower = conf_intervals[,1],
  CI_Upper = conf_intervals[,2]
)
print(or_table)
```

```{r}
# Visualizing important coefficients
coef_plot <- ggplot(or_table[-1,], aes(x = reorder(Variable, Odds_Ratio), y = Odds_Ratio)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Odds Ratios with 95% Confidence Intervals",
       x = "Variable", y = "Odds Ratio") +
  theme_minimal()
print(coef_plot)
```

To assess predictive accuracy, I calculated several metrics and used robust techniques to evaluate the model's performance. First, I computed the predicted probabilities for CHD using the final model and evaluated its discriminative ability with a Receiver Operating Characteristic (ROC) curve. The Area Under the Curve (AUC) was 0.776, indicating good overall predictive accuracy. To determine the optimal threshold for classification, I used Youden's J statistic, which identified 0.29 as the cutpoint that balances sensitivity and specificity.

Using this threshold, I created a confusion matrix, which revealed an accuracy of 69%, sensitivity of 83%, and specificity of 61%. While the model effectively identifies CHD cases (high sensitivity), it is less proficient at correctly classifying non-CHD cases (lower specificity). Precision was calculated at 53%, and the F1 score which is a harmonic mean of precision and sensitivity was 0.65, reflecting moderate performance in balancing false positives and false negatives.

To further validate the model, I performed 5-fold cross-validation using a simplified formula with key predictors (age, tobacco, LDL, and famhist). This yielded consistent results across folds, with an average AUC of 0.776, accuracy of 65%, sensitivity of 77%, and specificity of 59%. These metrics confirm that the model generalizes well to new data. Additionally, a learning curve analysis showed stable performance as training set size increased, suggesting that adding more data might not significantly improve accuracy.

Overall, the assessments that I demonstrated shows that the model performs reasonably well in predicting CHD status, with strong sensitivity but room for improvement in specificity and precision.





Explaining the Final Coronary Heart Disease Prediction Model

Part 1: Understanding the Coefficients (15 points)

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(broom)
library(knitr)

# Assuming final_model has been selected from previous analysis
chd_data <- read.csv("chd.csv")
chd_data$famhist <- as.factor(chd_data$famhist)

# Creating the final model (this should match your selected model from Part 1)
final_model <- glm(chd ~ age + tobacco + ldl + sbp + famhist + obesity,
                  data = chd_data, family = binomial)

# 1.1 Extract model summary and coefficients
model_summary <- summary(final_model)
coef_table <- tidy(final_model)
print(coef_table)
```
```{r}
# 1.2 Calculating and interpret odds ratios with confidence intervals
odds_ratios <- exp(coef(final_model))
conf_int <- exp(confint(final_model))

# Combine into a table with proper formatting
or_table <- data.frame(
  Variable = names(odds_ratios),
  Coefficient = coef(final_model),
  Odds_Ratio = odds_ratios,
  CI_Lower = conf_int[,1],
  CI_Upper = conf_int[,2]
)

# Formatting the table nicely
kable(or_table, 
      col.names = c("Variable", "Coefficient (Î²)", "Odds Ratio", "95% CI Lower", "95% CI Upper"),
      digits = 3,
      caption = "Coefficients and Odds Ratios for CHD Prediction Model")
```


```{r}
# 1.3 Calculating standardized coefficients to compare relative importance
# First standardize continuous predictors
predictors <- c("age", "tobacco", "ldl", "sbp", "obesity")
chd_data_std <- chd_data

for(var in predictors) {
  chd_data_std[[paste0(var, "_std")]] <- scale(chd_data[[var]])
}

# Fit model with standardized predictors
std_model <- glm(chd ~ age_std + tobacco_std + ldl_std + sbp_std + 
                   famhist + obesity_std,
                data = chd_data_std, family = binomial)

# Extract standardized coefficients
std_coef_table <- tidy(std_model)
print(std_coef_table)
```


```{r}
# 1.4 Calculating average marginal effects
library(margins)
marg_effects <- margins(final_model)
marg_summary <- summary(marg_effects)
print(marg_summary)

# Creating a table of average marginal effects
me_table <- data.frame(
  Variable = marg_summary$factor,
  AME = marg_summary$AME,
  SE = marg_summary$SE,
  p_value = marg_summary$p
)

kable(me_table, 
      col.names = c("Variable", "Average Marginal Effect", "Standard Error", "p-value"),
      digits = 3,
      caption = "Average Marginal Effects for CHD Prediction Model")
```


```{r}
# 1.5 Visualize odds ratios to better understand coefficients
or_plot <- ggplot(or_table[-1,], aes(x = reorder(Variable, Odds_Ratio), y = Odds_Ratio)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Odds Ratios with 95% Confidence Intervals",
       x = "Variable", 
       y = "Odds Ratio (log scale)") +
  scale_y_log10() +
  theme_minimal() +
  theme(axis.text = element_text(size = 10),
        plot.title = element_text(size = 14, face = "bold"))

print(or_plot)
```


```{r}
# 1.6 Calculating predicted probabilities for different values of key predictors
# Creating profiles for visualization
age_profile <- data.frame(
  age = seq(min(chd_data$age), max(chd_data$age), length.out = 100),
  tobacco = mean(chd_data$tobacco),
  ldl = mean(chd_data$ldl),
  sbp = mean(chd_data$sbp),
  famhist = factor("Absent", levels = levels(chd_data$famhist)),
  obesity = mean(chd_data$obesity)
)

age_profile$predicted_prob <- predict(final_model, newdata = age_profile, type = "response")

# Plot predicted probabilities vs. age
age_prob_plot <- ggplot(age_profile, aes(x = age, y = predicted_prob)) +
  geom_line(size = 1.2, color = "steelblue") +
  labs(title = "Predicted Probability of CHD by Age",
       x = "Age", y = "Probability of CHD") +
  theme_minimal()

print(age_prob_plot)
```



```{r}
# Doing the same for tobacco use
tobacco_profile <- data.frame(
  age = mean(chd_data$age),
  tobacco = seq(min(chd_data$tobacco), max(chd_data$tobacco), length.out = 100),
  ldl = mean(chd_data$ldl),
  sbp = mean(chd_data$sbp),
  famhist = factor("Absent", levels = levels(chd_data$famhist)),
  obesity = mean(chd_data$obesity)
)

tobacco_profile$predicted_prob <- predict(final_model, newdata = tobacco_profile, type = "response")

# Plotting predicted probabilities vs. tobacco use
tobacco_prob_plot <- ggplot(tobacco_profile, aes(x = tobacco, y = predicted_prob)) +
  geom_line(size = 1.2, color = "darkred") +
  labs(title = "Predicted Probability of CHD by Tobacco Use",
       x = "Cigarettes per Day", y = "Probability of CHD") +
  theme_minimal()

print(tobacco_prob_plot)
```

Ok so the coefficients in the logistic regression model represent the relationship between each predictor and the log odds of coronary heart disease (CHD). For example, the coefficient for age is 0.042, meaning that for every one-year increase in age, the log odds of CHD increase by 0.042. When converted to an odds ratio using $e^{\beta}$, this corresponds to an odds ratio of 1.043, indicating a 4.3\% increase in the odds of CHD per year of age. Similarly, tobacco has a coefficient of 0.086, translating to an odds ratio of 1.090, suggesting that each unit increase in tobacco consumption raises the odds of CHD by approximately 9\%.

The variable LDL cholesterol has a coefficient of 0.184 and an odds ratio of 1.202, implying that higher LDL levels significantly increase the likelihood of CHD. Family history (famhist) is particularly impactful, with a coefficient of 0.948 and an odds ratio of 2.582, meaning individuals with a positive family history are over two and a half times more likely to develop CHD compared to those without.

On the other hand, systolic blood pressure (sbp) and obesity have coefficients close to zero (0.007 and -0.034, respectively), indicating weaker or non-significant associations with CHD in this model. For instance, obesity's negative coefficient suggests a slight protective effect, but its p-value (0.258) shows that this relationship is not statistically significant.

Now to compare relative importance across predictors, standardized coefficients were calculated by scaling continuous variables to have mean zero and standard deviation one. This analysis revealed that age ($\beta=0.616$) and tobacco ($\beta=0.402$) are among the most influential predictors when adjusted for scale.

Finally, average marginal effects (AMEs) provided additional insights into how changes in predictors affect the probability of CHD directly. For example, age has an AME of 0.0073, meaning that each additional year increases the probability of CHD by approximately 0.73\%. Similarly, tobacco use increases CHD probability by about 1.48\% per unit increase.

These interpretations highlight both the magnitude and significance of predictors in influencing CHD risk, with family history emerging as a particularly strong factor alongside age and LDL cholesterol levels.


Part 2: Identifying Important Predictors (15 points)

```{r}
# Extract z-values from coefficient table
z_values <- abs(coef_table$statistic)
names(z_values) <- coef_table$term

# Remove intercept and sort by importance
z_values <- sort(z_values[-1], decreasing = TRUE)  
print("Variable importance based on z-values:")
print(z_values)
```

```{r}
# 2.1.2 Second approach: Calculate variable importance using AIC differences
# Extract model terms
model_vars <- attr(terms(final_model), "term.labels")

# Create function to compute AIC difference for a single variable
compute_aic_diff <- function(var, full_model, data) {
  # Get all other variables
  all_vars <- attr(terms(full_model), "term.labels")
  remaining_vars <- setdiff(all_vars, var)
  
  # Handle case with no remaining variables
  if (length(remaining_vars) == 0) {
    reduced_formula <- as.formula("chd ~ 1")
  } else {
    reduced_formula <- as.formula(
      paste("chd ~", paste(remaining_vars, collapse = " + "))
    )
  }
  
  # Fit reduced model
  reduced_model <- glm(reduced_formula, data = data, family = binomial)
  
  # Calculate AIC difference
  aic_diff <- AIC(reduced_model) - AIC(full_model)
  
  return(data.frame(Variable = var, AIC_Difference = aic_diff))
}
```

```{r}
# 2.1.3 Apply function to each variable
var_importance_list <- lapply(model_vars, compute_aic_diff, 
                             full_model = final_model, 
                             data = chd_data)

# Combine results
var_importance_df <- do.call(rbind, var_importance_list)

# Sort by importance (higher AIC difference = more important)
var_importance_df <- var_importance_df[order(-var_importance_df$AIC_Difference), ]

print("Variable importance based on AIC differences:")
print(var_importance_df)
```
```{r}
# 2.1.4 Create a clean table comparing both importance metrics
importance_table <- data.frame(
  Variable = var_importance_df$Variable,
  AIC_Difference = round(var_importance_df$AIC_Difference, 2),
  stringsAsFactors = FALSE  # Prevent conversion to factors
)

# Add Z-values with NA handling
importance_table$Z_Value <- NA  # Initialize with NA
for(i in 1:nrow(importance_table)) {
  var_name <- importance_table$Variable[i]
  if(var_name %in% names(z_values)) {
    importance_table$Z_Value[i] <- round(z_values[var_name], 2)
  }
}

# Add rankings for each metric (with NA handling)
importance_table$AIC_Rank <- rank(-importance_table$AIC_Difference)
importance_table$Z_Rank <- rank(-importance_table$Z_Value, na.last = "keep")

# Sort by AIC importance
importance_table <- importance_table[order(importance_table$AIC_Rank), ]
print("Comparison of variable importance metrics:")
print(importance_table)
```

```{r}
# 2.2 Variance Inflation Factors to check for multicollinearity
library(car)
vif_values <- vif(final_model)
print(vif_values)
```


```{r}
# 2.3 Creating a plot comparing different importance metrics
importance_comparison <- data.frame(
  Variable = names(z_values),
  Z_Value = z_values,
  AIC_Diff = var_importance_df$AIC_Difference[match(names(z_values), var_importance_df$Variable)]
)

# Normalizing values for comparison
importance_comparison$Z_Value_Norm <- importance_comparison$Z_Value / max(importance_comparison$Z_Value)
importance_comparison$AIC_Diff_Norm <- importance_comparison$AIC_Diff / max(importance_comparison$AIC_Diff)

# Reshaping for plotting
library(tidyr)
importance_long <- pivot_longer(
  importance_comparison, 
  cols = c(Z_Value_Norm, AIC_Diff_Norm),
  names_to = "Metric",
  values_to = "Value"
)

# Plotting comparison of variable importance metrics
importance_plot <- ggplot(importance_long, 
                         aes(x = reorder(Variable, Value), y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Variable Importance Using Different Metrics",
       x = "Variable",
       y = "Normalized Importance") +
  scale_fill_manual(values = c("steelblue", "darkred"),
                   labels = c("AIC Difference", "Z-Value")) +
  theme_minimal()

print(importance_plot)
```


```{r}
# 2.4 Predictive power - partial dependence plots for top predictors
library(pdp)

# Function to Creating a simple partial dependence plot
simple_pdp <- function(model, data, var) {
  # Creating sequence of values for the variable
  x_values <- seq(min(data[[var]]), max(data[[var]]), length.out = 50)
  
  # Creating prediction profiles
  profiles <- lapply(x_values, function(x) {
    new_data <- data
    new_data[[var]] <- x
    mean_pred <- mean(predict(model, newdata = new_data, type = "response"))
    return(data.frame(x = x, y = mean_pred))
  })
  
  # Combine results
  pdp_data <- do.call(rbind, profiles)
  names(pdp_data) <- c(var, "predicted_prob")
  
  # Creating plot
  plot <- ggplot(pdp_data, aes_string(x = var, y = "predicted_prob")) +
    geom_line(size = 1.2, color = "darkblue") +
    labs(
      title = paste("Partial Dependence Plot for", var),
      x = var,
      y = "Average Predicted Probability of CHD"
    ) +
    theme_minimal()
  
  return(plot)
}
```
```{r}
# Get top 3 important variables based on AIC difference
top_vars <- as.character(var_importance_df$Variable[1:3])
pdp_plots <- lapply(top_vars, function(var) {
  if (var != "famhist") {  # Skip categorical variables for this approach
    simple_pdp(final_model, chd_data, var)
  }
})

# Display the plots (removing NULL elements for categorical variables)
library(gridExtra)
pdp_plots <- pdp_plots[!sapply(pdp_plots, is.null)]
if (length(pdp_plots) > 0) {
  grid.arrange(grobs = pdp_plots, ncol = 2)
}
```

```{r}
# 2.5 Creating illustration of most important variables' effects
# Focus on top two continuous predictors
top_continuous_vars <- setdiff(top_vars, "famhist")[1:2]

# Creating profiles varying the top two predictors while holding others at mean/reference
grid_size <- 30
var1_seq <- seq(min(chd_data[[top_continuous_vars[1]]]), 
                max(chd_data[[top_continuous_vars[1]]]), 
                length.out = grid_size)
var2_seq <- seq(min(chd_data[[top_continuous_vars[2]]]), 
                max(chd_data[[top_continuous_vars[2]]]), 
                length.out = grid_size)

# Creating prediction grid
pred_grid <- expand.grid(
  var1 = var1_seq,
  var2 = var2_seq
)
names(pred_grid) <- top_continuous_vars

# Creating full prediction dataset
pred_data <- data.frame(
  pred_grid,
  age = mean(chd_data$age),
  tobacco = mean(chd_data$tobacco),
  ldl = mean(chd_data$ldl),
  sbp = mean(chd_data$sbp),
  famhist = factor("Absent", levels = levels(chd_data$famhist)),
  obesity = mean(chd_data$obesity)
)
```

```{r}
# Replicate the row for all combinations
pred_data <- pred_data[rep(1, nrow(pred_grid)), ]

# Add the grid values for the two variables we're varying
pred_data[[top_continuous_vars[1]]] <- pred_grid[[top_continuous_vars[1]]]
pred_data[[top_continuous_vars[2]]] <- pred_grid[[top_continuous_vars[2]]]

# Calculate predicted probabilities
pred_data$predicted_prob <- predict(final_model, newdata = pred_data, type = "response")

# Create the matrix for contour plotting
pred_matrix <- matrix(pred_data$predicted_prob, nrow = grid_size, ncol = grid_size)

# Creating contour plot data
contour_data <- expand.grid(
  x = var1_seq,
  y = var2_seq
)
```

```{r}
contour_data$z <- as.vector(pred_matrix)
names(contour_data) <- c("x", "y", "prob")
names(contour_data)[1:2] <- top_continuous_vars

contour_plot <- ggplot(contour_data, aes_string(x = top_continuous_vars[1], 
                                               y = top_continuous_vars[2],
                                               z = "prob")) +
  geom_contour_filled(bins = 10) +
  scale_fill_viridis_d() +
  labs(
    title = paste("Contour Plot of CHD Probability by", 
                top_continuous_vars[1], "and", top_continuous_vars[2]),
    x = top_continuous_vars[1],
    y = top_continuous_vars[2],
    fill = "Probability"
  ) +
  theme_minimal()

print(contour_plot)
```


Based on the results, several predictors stand out as particularly important for predicting coronary heart disease (CHD). So using two approaches i.e. z-values from the model coefficients and AIC differences from reduced models, I identified age, family history (famhist), and tobacco use as the most influential variables.

The z-values rank famhist as the most significant predictor, with a z-value of 3.99, indicating that having a family history of CHD strongly increases the likelihood of developing the condition. This aligns with its high AIC difference (14.25), suggesting that removing famhist from the model would significantly reduce its predictive power. Similarly, age has a z-value of 3.94 and an AIC difference of 14.26, highlighting its critical role in predicting CHD risk, as older individuals are more likely to develop the disease. Tobacco use also emerges as important, with a z-value of 3.20 and an AIC difference of 9.11, reflecting its strong association with increased CHD risk.

Other predictors, such as LDL cholesterol (ldl), systolic blood pressure (sbp), and obesity, appear less impactful based on both metrics. For instance, sbp and obesity have negative AIC differences (-0.35 and -0.70), suggesting that their removal might slightly improve the model's fit.

The partial dependence plots further illustrate the importance of these top predictors by showing how changes in age, tobacco use, and ldl influence the predicted probability of CHD. These visualizations confirm that increasing values for these predictors substantially raise CHD risk.

In summary, I chose these predictors because they consistently demonstrated strong statistical significance (high z-values) and substantial contributions to model performance (high AIC differences). Their biological relevance further supports their inclusion, as age, family history, and tobacco use are well-established risk factors for CHD.


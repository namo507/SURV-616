---
title: "Homework 1: Namit Shrivastava"
format: pdf
editor: visual
---
Firstly I will be creating the dataset
```{r}
film_noir_data <- data.frame(
  Week = as.Date(c("2022-10-02", "2022-10-09", "2022-10-16", "2022-10-23", 
                   "2022-10-30", "2022-11-06", "2022-11-13", "2022-11-20",
                   "2022-11-27", "2022-12-04", "2022-12-11", "2022-12-18")),
  Searches = c(68, 73, 58, 59, 72, 70, 77, 57, 56, 76, 63, 52)
)
```
## 1. a) [5 points] Calculate maximum likelihood estimate of p (i.e. the proportion of all 781 searches that occurred in each week). Graph these 12 proportions.

Ok so by formula maximum likelihood estimate (MLE) of p for each week is simply the proportion of searches in that week (each row) relative to the total searches i.e. 
$p_{i}$ = (searches in week i)/total searches
For example, in the table, for Week 10/2/2022, the p value will be calculated as 68/781 = 0.0871 and so on for the other rows. The results are:
```{r}
#Now I will be calculating the 
#proportions with formatted strings
total_searches <- sum(film_noir_data$Searches)
film_noir_data$Proportion <- sprintf("%d/%d = %.4f", 
                                   film_noir_data$Searches, 
                                   total_searches,
                                   film_noir_data$Searches/total_searches)

film_noir_data
```
As for the graph, the code is:
```{r}
#Importing the necessary library
library(ggplot2)

# Creating the plot using the existing data frame
ggplot(film_noir_data, aes(x = Week, y = Searches/total_searches)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = sprintf("%.3f", Searches/total_searches)), 
            vjust = -0.5) +
  labs(title = "Proportion of 'Film Noir' Searches by Week",
       y = "Proportion of Total Searches",
       x = "Week") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 week") +
  ylim(0, max(film_noir_data$Searches/total_searches) * 1.2)

```

## 1. b) [5 points] Write the null hypothesis that the proportion of searches for “film noir” is the same each week. Also, write the alternative hypothesis (i.e., that there has been a change in the proportion of searches each week).

Null Hypothesis ($H_{0}$): The proportion of searches for "film noir" is the same each week i.e., $p_{1}$ = $p_{2}$ = $p_{3}$ ..... = $p_{12}$ = 1/12 = 0.0833

Alternative Hypothesis ($H_{A}$): There has been a change in the proportion of searches each week i.e. basically it varies across weeks. So, $p_{i}$ != $p_{j}$, for at least one pair i,j or simply, at least one $p_{i}$ != 1/12

## 1. c) [15 points] Compute the X2 and G2 statistics. What do these tell us?

Chi-Square ($X^{2}$) Statistic is given as:
$$
X^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}
$$

where $O_{i}$ is the observed frequency (searches per week), and $E_{i}$ is the expected frequency under the null hypothesis, meaning the expected proportion for each week is equal since searches are distributed uniformly.

$E_{i}$ will be calculated as Total Searches/Number of weeks = 781/12 = 65.083
Now calculating the $O_{i}$ is given in the table as the searches value like $O_{1}$ = 68, $O_{2}$ = 73 and so on.
Now I will be calculating the value using this R code:

```{r}
# Observed frequencies 
O_i <- c(68, 73, 58, 59, 72, 70, 77, 57, 56, 76, 63, 52) 
# Expected frequency 
E_i <- rep(781 / 12, 12) 
# Calculate Chi-Square statistic 
chi_squared <- sum((O_i - E_i)^2 / E_i) 
chi_squared
```
So the Chi-Square statistic ($X^{2}$) is 12.52. This will be the hypothetical value. 
If the critical value from the chi-square table for degrees of freedom, df = 11 (since 12 weeks mentioned so 12-1 = 11) at 0.05 significance level is approximately 19.68.

Since 12.52 < 19.68, meaning the corresponding p-value is greater than 0.05, and hence we fail to reject the null hypothesis that the average weekly counts are all the same. 
In simple terms, there's no statistically significant evidence, at the 5% level, that the proportion of “film noir” searches varies across the 12 weeks.


Now calculating the Likelihood Ratio Test statistic, $G^{2}$, will measure how far the observed data deviate from the expected data under the null hypothesis. So in this question, it compares the observed weekly proportions of "film noir" searches to the expected uniform distribution (i.e., equal proportions across all weeks).

Now formulating, we have,
$$
G^2 = 2 \sum_{i=1}^{k} O_i \ln\!\biggl(\frac{O_i}{E_i}\biggr)
$$

Now I will be calculating the value using this R code:
```{r}
# Calculate G-Squared statistic 
G_squared <- 2 * sum(O_i * log(O_i / E_i)) 
G_squared
```
A $G^{2}$ value of 12.59 means for 12 categories (i.e., 11 degrees of freedom under the null hypothesis), it is like before, smaller than the critical value at the 5% level (approximately 19.68). 

This again implies the corresponding p-value is greater than 0.05, so we fail to reject the null hypothesis that the weekly “film noir” search proportions are equal. 

Simply one can say that there is no statistically significant evidence of week-to-week changes in the proportion of searches at the 5% level.

## 2. a) [5 points] Graph the proportions of all steps taken on each day of the week.
So the proportion will be calculated as the 
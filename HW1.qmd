---
title: "Homework 1: Namit Shrivastava"
format: pdf
editor: visual
---
Firstly I will be creating the dataset
```{r}
film_noir_data <- data.frame(
  Week = as.Date(c("2022-10-02", "2022-10-09", "2022-10-16", "2022-10-23", 
                   "2022-10-30", "2022-11-06", "2022-11-13", "2022-11-20",
                   "2022-11-27", "2022-12-04", "2022-12-11", "2022-12-18")),
  Searches = c(68, 73, 58, 59, 72, 70, 77, 57, 56, 76, 63, 52)
)
```

## 1. a) [5 points] Calculate maximum likelihood estimate of p (i.e. the proportion of all 781 searches that occurred in each week). Graph these 12 proportions.

Ok so by formula maximum likelihood estimate (MLE) of p for each week is simply the proportion of searches in that week (each row) relative to the total searches i.e. 
$p_{i}$ = (searches in week i)/total searches
For example, in the table, for Week 10/2/2022, the p value will be calculated as 68/781 = 0.0871 and so on for the other rows. The results are:
```{r}
# Now I will be calculating the 
# proportions with formatted strings
total_searches <- sum(film_noir_data$Searches)
film_noir_data$Proportion <- sprintf("%d/%d = %.4f", 
                                   film_noir_data$Searches, 
                                   total_searches,
                                   film_noir_data$Searches/total_searches)

film_noir_data
```

As for the graph, the code is:
```{r}
# Importing the necessary library
library(ggplot2)

# Creating the plot using the existing data frame
ggplot(film_noir_data, aes(x = Week, y = Searches/total_searches)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = sprintf("%.3f", Searches/total_searches)), 
            vjust = -0.5) +
  labs(title = "Proportion of 'Film Noir' Searches by Week",
       y = "Proportion of Total Searches",
       x = "Week") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 week") +
  ylim(0, max(film_noir_data$Searches/total_searches) * 1.2)

```

## 1. b) [5 points] Write the null hypothesis that the proportion of searches for “film noir” is the same each week. Also, write the alternative hypothesis (i.e., that there has been a change in the proportion of searches each week).

Null Hypothesis ($H_{0}$): The proportion of searches for "film noir" is the same each week i.e., $p_{1}$ = $p_{2}$ = $p_{3}$ ..... = $p_{12}$ = 1/12 = 0.0833

Alternative Hypothesis ($H_{A}$): There has been a change in the proportion of searches each week i.e. basically it varies across weeks. So, $p_{i} \neq p_{j}$, for at least one pair i,j or simply, at least one $p_{i} \neq 1/12$

## 1. c) [15 points] Compute the X2 and G2 statistics. What do these tell us?

Chi-Square ($X^{2}$) Statistic is given as:
$$
X^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}
$$
k=12 here

where $O_{i}$ is the observed frequency (searches per week), and $E_{i}$ is the expected frequency under the null hypothesis, meaning the expected proportion for each week is equal since searches are distributed uniformly.

$E_{i}$ will be calculated as Total Searches/Number of weeks = 781/12 = 65.083
Now calculating the $O_{i}$ is given in the table as the searches value like $O_{1}$ = 68, $O_{2}$ = 73 and so on.

Now I will be calculating the value using this R code:

```{r}
# Observed frequencies 
O_i <- c(68, 73, 58, 59, 72, 70, 77, 57, 56, 76, 63, 52) 
# Expected frequency 
E_i <- rep(781 / 12, 12) 
# Calculate Chi-Square statistic 
chi_squared <- sum((O_i - E_i)^2 / E_i) 
chi_squared
```

So the Chi-Square statistic ($X^{2}$) is 12.52. This will be the hypothetical value. 
If the critical value from the chi-square table for degrees of freedom, df = 11 (since 12 weeks mentioned so 12-1 = 11) at 0.05 significance level is approximately 19.68.

Since 12.52 < 19.68, meaning the corresponding p-value is greater than 0.05, and hence we fail to reject the null hypothesis that the average weekly counts are all the same. 
In simple terms, there's no statistically significant evidence, at the 5% level, that the proportion of “film noir” searches varies across the 12 weeks.


Now calculating the Likelihood Ratio Test statistic, $G^{2}$, will measure how far the observed data deviate from the expected data under the null hypothesis. So in this question, it compares the observed weekly proportions of "film noir" searches to the expected uniform distribution (i.e., equal proportions across all weeks).

Now formulating, we have,
$$
G^2 = 2 \sum_{i=1}^{k} O_i \ln\!\biggl(\frac{O_i}{E_i}\biggr)
$$
k=12 here

Now I will be calculating the value using this R code:
```{r}
# Calculate G-Squared statistic 
G_squared <- 2 * sum(O_i * log(O_i / E_i)) 
G_squared
```

A $G^{2}$ value of 12.59 means for 12 categories (i.e., 11 degrees of freedom under the null hypothesis), it is like before, smaller than the critical value at the 5% level (approximately 19.68). 

This again implies the corresponding p-value is greater than 0.05, so we fail to reject the null hypothesis that the weekly “film noir” search proportions are equal. 

Simply one can say that there is no statistically significant evidence of week-to-week changes in the proportion of searches at the 5% level.

Now showing the actual calculation of these values as:
[**Calculations:**]

| Week     | Search | Proportion                    | $\chi^2$ Calculation                      | $G^2$ Calculation                                       |
|---------------|---------------|---------------|---------------|---------------|
| 10/2/22  | 68     | $\frac{68}{781} = 0.08706786$ | $\frac{(68-65.0833)^2}{65.0833} = 0.1307$ | $2 \times 68 \times \ln(\frac{68}{65.0833}) = 5.9621$   |
| 10/9/22  | 73     | $\frac{73}{781} = 0.09346991$ | $\frac{(73-65.0833)^2}{65.0833} = 0.9630$ | $2 \times 73 \times \ln(\frac{73}{65.0833}) = 16.7595$  |
| 10/16/22 | 58     | $\frac{58}{781} = 0.07426376$ | $\frac{(58-65.0833)^2}{65.0833} = 0.7709$ | $2 \times 58 \times \ln(\frac{58}{65.0833}) = -13.3662$ |
| 10/23/22 | 59     | $\frac{59}{781} = 0.07554417$ | $\frac{(59-65.0833)^2}{65.0833} = 0.5686$ | $2 \times 59 \times \ln(\frac{59}{65.0833}) = -11.5795$ |
| 10/30/22 | 72     | $\frac{72}{781} = 0.09218950$ | $\frac{(72-65.0833)^2}{65.0833} = 0.7351$ | $2 \times 72 \times \ln(\frac{72}{65.0833}) = 14.5437$  |
| 11/6/22  | 70     | $\frac{70}{781} = 0.08962868$ | $\frac{(70-65.0833)^2}{65.0833} = 0.3714$ | $2 \times 70 \times \ln(\frac{70}{65.0833}) = 10.1957$  |
| 11/13/22 | 77     | $\frac{77}{781} = 0.09859155$ | $\frac{(77-65.0833)^2}{65.0833} = 2.1819$ | $2 \times 77 \times \ln(\frac{77}{65.0833}) = 25.8931$  |
| 11/20/22 | 57     | $\frac{57}{781} = 0.07298335$ | $\frac{(57-65.0833)^2}{65.0833} = 1.0039$ | $2 \times 57 \times \ln(\frac{57}{65.0833}) = -15.1184$ |
| 11/27/22 | 56     | $\frac{56}{781} = 0.07170294$ | $\frac{(56-65.0833)^2}{65.0833} = 1.2677$ | $2 \times 56 \times \ln(\frac{56}{65.0833}) = -16.8355$ |
| 12/4/22  | 76     | $\frac{76}{781} = 0.09731114$ | $\frac{(76-65.0833)^2}{65.0833} = 1.8311$ | $2 \times 76 \times \ln(\frac{76}{65.0833}) = 23.5699$  |
| 12/11/22 | 63     | $\frac{63}{781} = 0.08066581$ | $\frac{(63-65.0833)^2}{65.0833} = 0.0667$ | $2 \times 63 \times \ln(\frac{63}{65.0833}) = -4.0993$  |
| 12/18/22 | 52     | $\frac{52}{781} = 0.06658131$ | $\frac{(52-65.0833)^2}{65.0833} = 2.6301$ | $2 \times 52 \times \ln(\frac{52}{65.0833}) = -23.3402$ |

$\chi^2$ = 0.1307 + 0.9630 + 0.7709 + 0.5686 + 0.7351 + 0.3714 + 2.1819 + 1.0039 + 1.2677 + 1.8311 + 0.0667 + 2.6301 = 12.52

$G^2$ = 5.9621 + 16.7595 - 13.3662 - 11.5795 + 14.5437 + 10.1957 + 25.8931 - 15.1184 - 16.8355 + 23.5699 - 4.0993 - 23.3402 = 12.59

## 2. a) [5 points] Graph the proportions of all steps taken on each day of the week.
So the proportion will be calculated as the 

$p_{i} = \frac{n_{i}}{\text{Total Searches}}$
where i=1,2,…,7

So firstly let me create the data frame and calculate the proportions
```{r}
steps_data <- data.frame(
  Day = c("Sun", "Mon", "Tues", "Wed", "Thurs", "Fri", "Sat"),
  Steps = c(3358, 2894, 2346, 2981, 2956, 2239, 3974)
)
  # Calculate proportions
steps_data$Proportion <- steps_data$Steps / sum(steps_data$Steps)
steps_data
```

Now based on the proportion column, I can now show the plotting as:
```{r}
library(ggplot2)

# Providing data for steps_data
steps_data <- data.frame(
  Day = c("Sun", "Mon", "Tues", "Wed", "Thurs", "Fri", "Sat"),
  Steps = c(3358, 2894, 2346, 2981, 2956, 2239, 3974),
  Proportion = c(0.1618469, 0.1394833, 0.1130711, 0.1436765, 0.1424716, 0.1079140, 0.1915365)
)

# Ensure the days are in the correct order since
# ggplot does it alphabetically by default
steps_data$Day <- factor(steps_data$Day, levels = c("Sun", "Mon", "Tues", "Wed", "Thurs", "Fri", "Sat"))

# Plotting the graph
ggplot(steps_data, aes(x = Day, y = Proportion)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Proportion of Steps Taken Each Day", y = "Proportion of Total Steps") +
  geom_text(aes(label = round(Proportion, 3)), vjust = -0.5)
```

## 2. b) [10 points] Calculate the maximum likelihood estimate of p, as well as the maximum likelihood estimate of V(p). Note that the latter V(p) is a matrix of variances and covariances.

MLE of p will be given as $\hat{p}_i = \frac{n_i}{N}$
So these values are already calculated before. Now, for a multinomial distribution, the variances and covariances of $\hat{p}_i$ formulae are:

$$
\text{Var}(\hat{p}_i) = \frac{\hat{p}_i (1 - \hat{p}_i)}{N}
$$

and 

$$
\text{Cov}(\hat{p}_i, \hat{p}_j) = -\frac{\hat{p}_i \hat{p}_j}{N} \quad \text{for} \quad i \neq j
$$

So based on these formulae, I am gonna write the R code to get the values:
```{r}
# Calculated MLE of p from before
p_hat <- steps_data$Proportion
print(round(p_hat, 4))

# Now calculating variance-covariance matrix
n_total <- sum(steps_data$Steps)
V_p <- matrix(0, 7, 7)

# Filling the diagonal elements
for(i in 1:7) {
  V_p[i,i] <- (p_hat[i] * (1 - p_hat[i])) / n_total
}

# Then filling off-diagonal elements
for(i in 1:7) {
  for(j in 1:7) {
    if(i != j) {
      V_p[i,j] <- -(p_hat[i] * p_hat[j]) / n_total
    }
  }
}

V_p
```

So, based on the variance-covariance matrix results, one can infer that the variances of the daily proportions of steps are relatively small, indicating that the proportions are quite stable.

The diagonal elements of the matrix represent the variances of the proportions for each day, while the off-diagonal elements represent the covariances between the proportions of different days.

Interestingly, negative covariances suggest that an increase in the proportion of steps on one day is associated with a decrease in the proportion on another day which aligns with the graduate student's hypothesis that the number of steps taken each day should be about the same. Also, the small variances and negative covariances support the idea that the student's daily walking patterns are consistent, with no significant deviations from the expected proportions.

Now the ind depth calculation will be as follows:

[**Calculation:**] $$
\scalebox{0.6}{$
\tiny
\begin{aligned}
\hat{V}(\hat{p}) & = \frac{1}{20748} \left( 
\begin{bmatrix}
0.1618469 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0.1394833 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0.1130711 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0.1436765 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0.1424716 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0.1079140 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0.1915365
\end{bmatrix} - 
\begin{bmatrix}
0.1618469 \\ 0.1394833 \\ 0.1130711 \\ 0.1436765 \\ 0.1424716 \\ 0.1079140 \\ 0.1915365
\end{bmatrix}
\begin{bmatrix}
0.1618469 0.1394833 0.1130711 0.1436765 0.1424716 0.1079140 0.1915365
\end{bmatrix}
\right) \\
& \implies \frac{1}{20748} \left(
\begin{bmatrix}
0.1618469 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0.1394833 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0.1130711 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0.1436765 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0.1424716 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0.1079140 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0.1915365
\end{bmatrix} -
\begin{bmatrix}
0.02619443 & 0.02257495 & 0.01830022 & 0.02325360 & 0.02305858 & 0.01746555 & 0.03099960 \\
0.02257495 & 0.01945560 & 0.01577154 & 0.02004048 & 0.01987241 & 0.01505221 & 0.02671615 \\
0.01830022 & 0.01577154 & 0.01278508 & 0.01624567 & 0.01610942 & 0.01220196 & 0.02165725 \\
0.02325360 & 0.02004048 & 0.01624567 & 0.02064294 & 0.02046982 & 0.01550471 & 0.02751930 \\
0.02305858 & 0.01987241 & 0.01610942 & 0.02046982 & 0.02029815 & 0.01537468 & 0.02728851 \\
0.01746555 & 0.01505221 & 0.01220196 & 0.01550471 & 0.01537468 & 0.01164543 & 0.02066948 \\
0.03099960 & 0.02671615 & 0.02165725 & 0.02751930 & 0.02728851 & 0.02066948 & 0.03668624
\end{bmatrix}
\right) \\
& \implies \frac{1}{20748} \begin{bmatrix}
0.02619443 & 0.02257495 & 0.01830022 & 0.02325360 & 0.02305858 & 0.01746555 & 0.03099960 \\
0.02257495 & 0.01945560 & 0.01577154 & 0.02004048 & 0.01987241 & 0.01505221 & 0.02671615 \\
0.01830022 & 0.01577154 & 0.01278508 & 0.01624567 & 0.01610942 & 0.01220196 & 0.02165725 \\
0.02325360 & 0.02004048 & 0.01624567 & 0.02064294 & 0.02046982 & 0.01550471 & 0.02751930 \\
0.02305858 & 0.01987241 & 0.01610942 & 0.02046982 & 0.02029815 & 0.01537468 & 0.02728851 \\
0.01746555 & 0.01505221 & 0.01220196 & 0.01550471 & 0.01537468 & 0.01164543 & 0.02066948 \\
0.03099960 & 0.02671615 & 0.02165725 & 0.02751930 & 0.02728851 & 0.02066948 & 0.03668624
\end{bmatrix}\\
& \implies
\begin{bmatrix}
6.5381 \times 10^{-6} & -1.0881 \times 10^{-6} & -8.8202 \times 10^{-7} & -1.1208 \times 10^{-6} & -1.1114 \times 10^{-6} & -8.4179 \times 10^{-7} & -1.4941 \times 10^{-6} \\
-1.0881 \times 10^{-6} & 5.7850 \times 10^{-6} & -7.6015 \times 10^{-7} & -9.6590 \times 10^{-7} & -9.5780 \times 10^{-7} & -7.2548 \times 10^{-7} & -1.2877 \times 10^{-6} \\
-8.8202 \times 10^{-7} & -7.6015 \times 10^{-7} & 4.8335 \times 10^{-6} & -7.8300 \times 10^{-7} & -7.7643 \times 10^{-7} & -5.8810 \times 10^{-7} & -1.0438 \times 10^{-6} \\
-1.1208 \times 10^{-6} & -9.6590 \times 10^{-7} & -7.8300 \times 10^{-7} & 5.9299 \times 10^{-6} & -9.8659 \times 10^{-7} & -7.4729 \times 10^{-7} & -1.3264 \times 10^{-6} \\
-1.1114 \times 10^{-6} & -9.5780 \times 10^{-7} & -7.7643 \times 10^{-7} & -9.8659 \times 10^{-7} & 5.8884 \times 10^{-6} & -7.4102 \times 10^{-7} & -1.3152 \times 10^{-6} \\
-8.4179 \times 10^{-7} & -7.2548 \times 10^{-7} & -5.8810 \times 10^{-7} & -7.4729 \times 10^{-7} & -7.4102 \times 10^{-7} & 4.6399 \times 10^{-6} & -9.9622 \times 10^{-7} \\
-1.4941 \times 10^{-6} & -1.2877 \times 10^{-6} & -1.0438 \times 10^{-6} & -1.3264 \times 10^{-6} & -1.3152 \times 10^{-6} & -9.9622 \times 10^{-7} & 7.4634 \times 10^{-6}
\end{bmatrix}
\end{aligned}$}$$

## 2. c) [15 points] Calculate the maximum likelihood estimate of the proportion of steps taken on the weekend(Sunday and Saturday, p1+p7) and the maximum likelihood estimate of the variance of the proportion of steps taken on the weekend.

So the MLE of the proportion for weekend is $\hat{p}_{\text{weekend}} = \hat{p}_{\text{Sunday}} + \hat{p}_{\text{Saturday}}$
So calculated value will be:
```{r}
p_weekend <- p_hat[1] + p_hat[7]  # Sunday + Saturday
p_weekend
```

Based on the result, the maximum likelihood estimate of the proportion of steps taken on the weekend (Sunday and Saturday) is approximately 0.3534. This means that about 35.34% of the total steps taken in a week are taken on these two days. This result tells me that the student tends to walk more on the weekends compared to the weekdays, which could be due to having more free time or engaging in more activities that involve walking.

Meanwhile the MLE of the variance of the proportion of steps taken on the weekend will be $\text{Var}(\hat{p}_{\text{weekend}}) = \text{Var}(\hat{p}_{\text{Sunday}}) + \text{Var}(\hat{p}_{\text{Saturday}}) + 2 \times \text{Cov}(\hat{p}_{\text{Sunday}}, \hat{p}_{\text{Saturday}})$

So calculated value will be:
```{r}
var_weekend <- V_p[1,1] + V_p[7,7] + 2*V_p[1,7]
var_weekend
```

Now the variance of the proportion of steps taken on the weekend is approx 1.101328e-05. This small variance indicates that the proportion of steps taken on the weekend is quite stable and consistent. Not only that but the low variance suggests that there is not much fluctuation in the student's walking pattern on the weekends, reinforcing the idea that the student consistently walks more on these days. Hence, this consistency could be due to a regular routine or planned activities that involve walking during the weekends.

## 2. d) [15 Points] Test the hypothesis that: H_0: p_1 = p_2 = p_3 = p_4 = p_5 = p_6 = p_7 versus H_A: p_1 \neq p_2 \neq p_3 \neq p_4 \neq p_5 \neq p_6 \neq p_7$$ by computing both the X^2 and G^2 statistics. What do you conclude?

So, under the null hypothesis, the number of steps taken each day is expected to be equal.

Chi-Square ($X^{2}$) Statistic is given as:
$$
X^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}
$$
k=7 here

Here, $O_{i}$ is is the observed number of steps for day i and 
$E_{i}$ will be calculated as N/7, where N is the total number of steps (20,748).
So $E_i = \frac{20,748}{7} \approx 2,964$

Now the R code for this calculation is:
```{r}
# Observed steps from the table
steps <- c(3358, 2894, 2346, 2981, 2956, 2239, 3974)

# Expected steps (equal for each day based
# on the null hypothesis)
expected_steps <- rep(20748 / 7, 7)

# Calculating Chi-Square statistic
chi_squared <- sum((steps - expected_steps)^2 / expected_steps)
chi_squared
```

Now, under the null hypothesis that steps are evenly distributed across days, one would expect about 2,964 steps per day. The large $X^{2}$ value indicates substantial deviations from this expectation. With 6 degrees of freedom (since 7 days - 1), the critical value at alpha = 0.05 would be approximately 12.59. But the calculated value of 704.5 far exceeds this threshold.
This provides very strong statistical evidence against the null hypothesis.

Looking at the proportions graph from before, one can see why there's a substantial variation across days, with Saturday (0.192) and Sunday (0.162) showing notably higher proportions than weekdays, and Friday having the lowest proportion (0.108). The magnitude of $X^{2}$ suggests these differences are not due to random chance but represent real, systematic variations in the student's walking patterns between different days of the week.


Now calculating the Likelihood Ratio Test statistic, $G^{2}$, will measure how far the observed data deviate from the expected data under the null hypothesis. So in this question, it tests whether the observed step counts significantly differ from the expected counts under the null hypothesis of equal daily proportions.

Now formulating, we have,
$$
G^2 = 2 \sum_{i=1}^{k} O_i \ln\!\biggl(\frac{O_i}{E_i}\biggr)
$$
k=7 here

So the R code for this is:
```{r}
# Calculate G-Squared statistic
G_squared <- 2 * sum(steps * log(steps / expected_steps))
G_squared
```

Again, with 6 degrees of freedom (7 days - 1), the critical value at alpha = 0.05 is approximately 12.59. the $G^{2}$ value of 695.3545 vastly exceeds this threshold.

This extremely large G2 value indicates substantial variation in the student's walking patterns across different days. The weekend days, particularly Saturday with 3,974 steps and Sunday with 3,358 steps show markedly different patterns compared to weekdays, with Friday having the lowest count at 2,239 steps. Also proportion values were shown previously in the graph.

Hence, both G2 (695.3545) and X2 (704.5) statistics lead to the same conclusion that one must reject the null hypothesis that the student's daily step counts are uniformly distributed across the week. The data overall strongly suggests that the student's walking behavior varies significantly by day of the week.

[**Calculation:**]

Now I am considering all the proportion to be same for the null hypothesis. So the expected proportion will be given as

$E = \frac{\text{total steps}}{\text{total days}} = \frac{20748}{7} = 2964$

Now to calculate $X^{2}$ and $G^{2}$ values like before

```{=tex}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Days} & \textbf{Steps} & \textbf{Proportion ($P$)} & \textbf{$x^2$} & \textbf{$G^2$} \\ \hline
Sun & 3358 & $\frac{3358}{20748} = 0.1618469$ & $\frac{(3358-2964)^2}{2964} = 52.37381916$ & $2 \times 3358 \ln\left(\frac{3358}{2964}\right) = 838.1961$ \\ \hline
Mon & 2894 & $\frac{2894}{20748} = 0.1394833$ & $\frac{(2894-2964)^2}{2964} = 1.65317139$ & $2 \times 2894 \ln\left(\frac{2894}{2964}\right) = -138.33366$ \\ \hline
Tue & 2346 & $\frac{2346}{20748} = 0.1130711$ & $\frac{(2346-2964)^2}{2964} = 128.854251$ & $2 \times 2346 \ln\left(\frac{2346}{2964}\right) = -1097.12078$ \\ \hline
Wed & 2981 & $\frac{2981}{20748} = 0.1436765$ & $\frac{(2981-2964)^2}{2964} = 0.09750337$ & $2 \times 2981 \ln\left(\frac{2981}{2964}\right) = 34.09732$ \\ \hline
Thu & 2956 & $\frac{2956}{20748} = 0.1424716$ & $\frac{(2956-2964)^2}{2964} = 0.02159244$ & $2 \times 2956 \ln\left(\frac{2956}{2964}\right) = -15.97839$ \\ \hline
Fri & 2239 & $\frac{2239}{20748} = 0.107914$ & $\frac{(2239-2964)^2}{2964} = 177.3363698$ & $2 \times 2239 \ln\left(\frac{2239}{2964}\right) = -1256.12544$ \\ \hline
Sat & 3974 & $\frac{3974}{20748} = 0.1915365$ & $\frac{(3974-2964)^2}{2964} = 344.1632929$ & $2 \times 3974 \ln\left(\frac{3974}{2964}\right) = 2330.61935$ \\ \hline
\end{tabular}
\label{tab:step_data}
\end{table}
```
$\chi^2=52.37+1.65+128.85+0.1+0.02+177.34+344.16 = 704.5$

$G^2 = 838.2+-138.33+-1097.12+34.1+-15.98+-1256.13+2330.6 = 695.35$

## 3. a) [5 points] About 1.27% (n11+n21)/(n11+n21+n12+22) had myocardial infarction. Since this was a designed experiment, 50% were assigned to take a placebo. If the use of aspirin or placebo was independent of risk of myocardial infarction (i.e. if the risk of myocardial infarction was no different whether you took placebo or aspirin), what would the expected counts be in each cell (n11, n12, n21, and n22)?

So firstly I will create a table which includes the total row and column:

```{r}
# Creating the observed data as a matrix
observed_data <- matrix(c(173, 9879, 83, 9970), 
                        nrow = 2, 
                        byrow = TRUE, 
                        dimnames = list(Group = c("Placebo", "Aspirin"), 
                                        MI_Status = c("Yes (MI)", "No (No MI)")))

# Now adding a new column for row totals
observed_data_with_totals <- cbind(observed_data, Total = rowSums(observed_data))

# Now adding a new row for column totals
observed_data_with_totals <- rbind(observed_data_with_totals, 
                                   Total = colSums(observed_data_with_totals))
observed_data_with_totals
```

Total number of participants with myocardial infarction (Yes) will be $n_{11} + n_{21} = 173 + 83 = 256$

Total number of participants without myocardial infarction (No) is $n_{12} + n_{22} = 9879 + 9970 = 19849$

Total number of participants in the Placebo group turns out to be $n_{11} + n_{12} = 173 + 9879 = 10052$

Total number of participants in the Aspirin group will be $n_{21} + n_{22} = 83 + 9970 = 10053$

Grand total of participants is simply $n_{11} + n_{12} + n_{21} + n_{22} = 256 + 19849 = 20105$

Now to find the expected counts in each cell (n11, n12, n21, and n22) under the assumption that the use of aspirin or placebo was independent of the risk of myocardial infarction, I will be using this general formula:

$$E_{ij} = \frac{(\text{row total}_i) \times (\text{column total}_j)}{\text{grand total}}$$

Hence, the R code for expected counts for each cell will be:

```{r}
# Defining the observed counts
n11 <- 173
n12 <- 9879
n21 <- 83
n22 <- 9970

# Calculating the marginal totals
total_MI <- n11 + n21
total_no_MI <- n12 + n22
total_placebo <- n11 + n12
total_aspirin <- n21 + n22
grand_total <- n11 + n12 + n21 + n22

# Now calculating the expected counts
E11 <- (total_placebo * total_MI) / grand_total
E12 <- (total_placebo * total_no_MI) / grand_total
E21 <- (total_aspirin * total_MI) / grand_total
E22 <- (total_aspirin * total_no_MI) / grand_total

E11
E12
E21
E22
```

So for each cell, the expected counts is simply calculated as:

$$E_{11} = \frac{(173 + 9879) \times (173 + 83)}{173 + 9879 + 83 + 9970} = \frac{10052 \times 256}{20105} = 127.9936$$

$$E_{12} = \frac{(173 + 9879) \times (9879 + 9970)}{173 + 9879 + 83 + 9970} = \frac{10052 \times 19849}{20105} = 9924.006$$

$$E_{21} = \frac{(83 + 9970) \times (173 + 83)}{173 + 9879 + 83 + 9970} = \frac{10053 \times 256}{20105} = 128.0064$$

$$E_{22} = \frac{(83 + 9970) \times (9879 + 9970)}{173 + 9879 + 83 + 9970} = \frac{10053 \times 19849}{20105} = 9924.994$$
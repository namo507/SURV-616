---
title: "Homework 1: Namit Shrivastava"
format: pdf
editor: visual
---
Firstly I will be creating the dataset
```{r}
film_noir_data <- data.frame(
  Week = as.Date(c("2022-10-02", "2022-10-09", "2022-10-16", "2022-10-23", 
                   "2022-10-30", "2022-11-06", "2022-11-13", "2022-11-20",
                   "2022-11-27", "2022-12-04", "2022-12-11", "2022-12-18")),
  Searches = c(68, 73, 58, 59, 72, 70, 77, 57, 56, 76, 63, 52)
)
```

## 1. a) [5 points] Calculate maximum likelihood estimate of p (i.e. the proportion of all 781 searches that occurred in each week). Graph these 12 proportions.

Ok so by formula maximum likelihood estimate (MLE) of p for each week is simply the proportion of searches in that week (each row) relative to the total searches i.e. 
$p_{i}$ = (searches in week i)/total searches
For example, in the table, for Week 10/2/2022, the p value will be calculated as 68/781 = 0.0871 and so on for the other rows. The results are:
```{r}
#Now I will be calculating the 
#proportions with formatted strings
total_searches <- sum(film_noir_data$Searches)
film_noir_data$Proportion <- sprintf("%d/%d = %.4f", 
                                   film_noir_data$Searches, 
                                   total_searches,
                                   film_noir_data$Searches/total_searches)

film_noir_data
```

As for the graph, the code is:
```{r}
#Importing the necessary library
library(ggplot2)

# Creating the plot using the existing data frame
ggplot(film_noir_data, aes(x = Week, y = Searches/total_searches)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = sprintf("%.3f", Searches/total_searches)), 
            vjust = -0.5) +
  labs(title = "Proportion of 'Film Noir' Searches by Week",
       y = "Proportion of Total Searches",
       x = "Week") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 week") +
  ylim(0, max(film_noir_data$Searches/total_searches) * 1.2)

```

## 1. b) [5 points] Write the null hypothesis that the proportion of searches for “film noir” is the same each week. Also, write the alternative hypothesis (i.e., that there has been a change in the proportion of searches each week).

Null Hypothesis ($H_{0}$): The proportion of searches for "film noir" is the same each week i.e., $p_{1}$ = $p_{2}$ = $p_{3}$ ..... = $p_{12}$ = 1/12 = 0.0833

Alternative Hypothesis ($H_{A}$): There has been a change in the proportion of searches each week i.e. basically it varies across weeks. So, $p_{i}$ != $p_{j}$, for at least one pair i,j or simply, at least one $p_{i}$ != 1/12

## 1. c) [15 points] Compute the X2 and G2 statistics. What do these tell us?

Chi-Square ($X^{2}$) Statistic is given as:
$$
X^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}
$$

where $O_{i}$ is the observed frequency (searches per week), and $E_{i}$ is the expected frequency under the null hypothesis, meaning the expected proportion for each week is equal since searches are distributed uniformly.

$E_{i}$ will be calculated as Total Searches/Number of weeks = 781/12 = 65.083
Now calculating the $O_{i}$ is given in the table as the searches value like $O_{1}$ = 68, $O_{2}$ = 73 and so on.
Now I will be calculating the value using this R code:

```{r}
# Observed frequencies 
O_i <- c(68, 73, 58, 59, 72, 70, 77, 57, 56, 76, 63, 52) 
# Expected frequency 
E_i <- rep(781 / 12, 12) 
# Calculate Chi-Square statistic 
chi_squared <- sum((O_i - E_i)^2 / E_i) 
chi_squared
```

So the Chi-Square statistic ($X^{2}$) is 12.52. This will be the hypothetical value. 
If the critical value from the chi-square table for degrees of freedom, df = 11 (since 12 weeks mentioned so 12-1 = 11) at 0.05 significance level is approximately 19.68.

Since 12.52 < 19.68, meaning the corresponding p-value is greater than 0.05, and hence we fail to reject the null hypothesis that the average weekly counts are all the same. 
In simple terms, there's no statistically significant evidence, at the 5% level, that the proportion of “film noir” searches varies across the 12 weeks.


Now calculating the Likelihood Ratio Test statistic, $G^{2}$, will measure how far the observed data deviate from the expected data under the null hypothesis. So in this question, it compares the observed weekly proportions of "film noir" searches to the expected uniform distribution (i.e., equal proportions across all weeks).

Now formulating, we have,
$$
G^2 = 2 \sum_{i=1}^{k} O_i \ln\!\biggl(\frac{O_i}{E_i}\biggr)
$$

Now I will be calculating the value using this R code:
```{r}
# Calculate G-Squared statistic 
G_squared <- 2 * sum(O_i * log(O_i / E_i)) 
G_squared
```

A $G^{2}$ value of 12.59 means for 12 categories (i.e., 11 degrees of freedom under the null hypothesis), it is like before, smaller than the critical value at the 5% level (approximately 19.68). 

This again implies the corresponding p-value is greater than 0.05, so we fail to reject the null hypothesis that the weekly “film noir” search proportions are equal. 

Simply one can say that there is no statistically significant evidence of week-to-week changes in the proportion of searches at the 5% level.

## 2. a) [5 points] Graph the proportions of all steps taken on each day of the week.
So the proportion will be calculated as the 

$p_{i} = \frac{n_{i}}{\text{Total Searches}}$
where i=1,2,…,7

So firstly let me create the data frame and calculate the proportions
```{r}
steps_data <- data.frame(
  Day = c("Sun", "Mon", "Tues", "Wed", "Thurs", "Fri", "Sat"),
  Steps = c(3358, 2894, 2346, 2981, 2956, 2239, 3974)
)
  # Calculate proportions
steps_data$Proportion <- steps_data$Steps / sum(steps_data$Steps)
steps_data
```

Now based on the proportion column, I can now show the plotting as:
```{r}
library(ggplot2)

# Providing data for steps_data
steps_data <- data.frame(
  Day = c("Sun", "Mon", "Tues", "Wed", "Thurs", "Fri", "Sat"),
  Steps = c(3358, 2894, 2346, 2981, 2956, 2239, 3974),
  Proportion = c(0.1618469, 0.1394833, 0.1130711, 0.1436765, 0.1424716, 0.1079140, 0.1915365)
)

# Ensure the days are in the correct order since
#ggplot does it alphabetically by default
steps_data$Day <- factor(steps_data$Day, levels = c("Sun", "Mon", "Tues", "Wed", "Thurs", "Fri", "Sat"))

# Plotting the graph
ggplot(steps_data, aes(x = Day, y = Proportion)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Proportion of Steps Taken Each Day", y = "Proportion of Total Steps") +
  geom_text(aes(label = round(Proportion, 3)), vjust = -0.5)
```

## 2. b) [10 points] Calculate the maximum likelihood estimate of p, as well as the maximum likelihood estimate of V(p). Note that the latter V(p) is a matrix of variances and covariances.

MLE of p will be given as $\hat{p}_i = \frac{n_i}{N}$
So these values are already calculated before. Now, for a multinomial distribution, the variances and covariances of $\hat{p}_i$ formulae are:

$$
\text{Var}(\hat{p}_i) = \frac{\hat{p}_i (1 - \hat{p}_i)}{N}
$$

and 

$$
\text{Cov}(\hat{p}_i, \hat{p}_j) = -\frac{\hat{p}_i \hat{p}_j}{N} \quad \text{for} \quad i \neq j
$$

So based on these formulae, I am gonna write the R code to get the values:
```{r}
# Calculated MLE of p from before
p_hat <- steps_data$Proportion
print(round(p_hat, 4))

# Now calculating variance-covariance matrix
n_total <- sum(steps_data$Steps)
V_p <- matrix(0, 7, 7)

# Filling the diagonal elements
for(i in 1:7) {
  V_p[i,i] <- (p_hat[i] * (1 - p_hat[i])) / n_total
}

# Then filling off-diagonal elements
for(i in 1:7) {
  for(j in 1:7) {
    if(i != j) {
      V_p[i,j] <- -(p_hat[i] * p_hat[j]) / n_total
    }
  }
}

V_p
```

So, based on the variance-covariance matrix results, one can infer that the variances of the daily proportions of steps are relatively small, indicating that the proportions are quite stable.

The diagonal elements of the matrix represent the variances of the proportions for each day, while the off-diagonal elements represent the covariances between the proportions of different days.

Interestingly, negative covariances suggest that an increase in the proportion of steps on one day is associated with a decrease in the proportion on another day which aligns with the graduate student's hypothesis that the number of steps taken each day should be about the same. Also, the small variances and negative covariances support the idea that the student's daily walking patterns are consistent, with no significant deviations from the expected proportions.

## 2. c) [15 points] Calculate the maximum likelihood estimate of the proportion of steps taken on the weekend(Sunday and Saturday, p1+p7) and the maximum likelihood estimate of the variance of the proportion of steps taken on the weekend.

So the MLE of the proportion for weekend is $\hat{p}_{\text{weekend}} = \hat{p}_{\text{Sunday}} + \hat{p}_{\text{Saturday}}$
So calculated value will be:
```{r}
p_weekend <- p_hat[1] + p_hat[7]  # Sunday + Saturday
p_weekend
```

Based on the result, the maximum likelihood estimate of the proportion of steps taken on the weekend (Sunday and Saturday) is approximately 0.3534. This means that about 35.34% of the total steps taken in a week are taken on these two days. This result tells me that the student tends to walk more on the weekends compared to the weekdays, which could be due to having more free time or engaging in more activities that involve walking.

Meanwhile the MLE of the variance of the proportion of steps taken on the weekend will be $\text{Var}(\hat{p}_{\text{weekend}}) = \text{Var}(\hat{p}_{\text{Sunday}}) + \text{Var}(\hat{p}_{\text{Saturday}}) + 2 \times \text{Cov}(\hat{p}_{\text{Sunday}}, \hat{p}_{\text{Saturday}})$

So calculated value will be:
```{r}
var_weekend <- V_p[1,1] + V_p[7,7] + 2*V_p[1,7]
var_weekend
```

Now the variance of the proportion of steps taken on the weekend is approx 1.101328e-05. This small variance indicates that the proportion of steps taken on the weekend is quite stable and consistent. Not only that but the low variance suggests that there is not much fluctuation in the student's walking pattern on the weekends, reinforcing the idea that the student consistently walks more on these days. Hence, this consistency could be due to a regular routine or planned activities that involve walking during the weekends.

## 3. a) [5 points] About 1.27% (n11+n21)/(n11+n21+n12+22) had myocardial infarction. Since this was a designed experiment, 50% were assigned to take a placebo. If the use of aspirin or placebo was independent of risk of myocardial infarction (i.e. if the risk of myocardial infarction was no different whether you took placebo or aspirin), what would the expected counts be in each cell (n11, n12, n21, and n22)?

So firstly I will create a table which includes the total row and column:

```{r}
# Creating the observed data as a matrix
observed_data <- matrix(c(173, 9879, 83, 9970), 
                        nrow = 2, 
                        byrow = TRUE, 
                        dimnames = list(Group = c("Placebo", "Aspirin"), 
                                        MI_Status = c("Yes (MI)", "No (No MI)")))

# Now adding a new column for row totals
observed_data_with_totals <- cbind(observed_data, Total = rowSums(observed_data))

# Now adding a new row for column totals
observed_data_with_totals <- rbind(observed_data_with_totals, 
                                   Total = colSums(observed_data_with_totals))
observed_data_with_totals
```

Total number of participants with myocardial infarction (Yes) will be $n_{11} + n_{21} = 173 + 83 = 256$

Total number of participants without myocardial infarction (No) is $n_{12} + n_{22} = 9879 + 9970 = 19849$

Total number of participants in the Placebo group turns out to be $n_{11} + n_{12} = 173 + 9879 = 10052$

Total number of participants in the Aspirin group will be $n_{21} + n_{22} = 83 + 9970 = 10053$

Grand total of participants is simply $n_{11} + n_{12} + n_{21} + n_{22} = 256 + 19849 = 20105$

Now to find the expected counts in each cell (n11, n12, n21, and n22) under the assumption that the use of aspirin or placebo was independent of the risk of myocardial infarction, I will be using this general formula:

$$E_{ij} = \frac{(\text{row total}_i) \times (\text{column total}_j)}{\text{grand total}}$$

Hence, the R code for expected counts for each cell will be:

```{r}
# Defining the observed counts
n11 <- 173
n12 <- 9879
n21 <- 83
n22 <- 9970

# Calculating the marginal totals
total_MI <- n11 + n21
total_no_MI <- n12 + n22
total_placebo <- n11 + n12
total_aspirin <- n21 + n22
grand_total <- n11 + n12 + n21 + n22

# Now calculating the expected counts
E11 <- (total_placebo * total_MI) / grand_total
E12 <- (total_placebo * total_no_MI) / grand_total
E21 <- (total_aspirin * total_MI) / grand_total
E22 <- (total_aspirin * total_no_MI) / grand_total

E11
E12
E21
E22
```

So for each cell, the expected counts is simply calculated as:

$$E_{11} = \frac{(n_{11} + n_{12}) \times (n_{11} + n_{21})}{n_{11} + n_{12} + n_{21} + n_{22}} = 127.9936$$

$$E_{12} = \frac{(n_{11} + n_{12}) \times (n_{12} + n_{22})}{n_{11} + n_{12} + n_{21} + n_{22}} = 9924.006$$

$$E_{21} = \frac{(n_{21} + n_{22}) \times (n_{11} + n_{21})}{n_{11} + n_{12} + n_{21} + n_{22}} = 128.0064$$

$$E_{22} = \frac{(n_{21} + n_{22}) \times (n_{12} + n_{22})}{n_{11} + n_{12} + n_{21} + n_{22}} = 9924.994$$